目录：
[TOC]
####1.Validation Curve（验证曲线）
by lcx

![1](https://raw.githubusercontent.com/6studentsfromsspku/sspku-300-concepts/master/卡片集合/卡片-206.jpg)
- 验证曲线的作用：
  我们知道误差由偏差(bias)、方差(variance)和噪声(noise)组成。

  - 偏差：模型对于不同的训练样本集，预测结果的平均误差。

  - 方差：模型对于不同训练样本集的敏感程度。

  - 噪声：数据集本身的一项属性。

- 同样的数据（cos函数上的点加上噪声），我们用同样的模型（polynomial），但是超参数却不同（degree ＝ 1, 4 ，15），会得到不同的拟合效果：

![验证曲线——1](https://raw.githubusercontent.com/6studentsfromsspku/sspku-300-concepts/master/lcx引用图片/验证曲线_1.png)

- 第一个模型太简单，模型本身就拟合不了这些数据（高偏差）；

- 第二个模型可以看成几乎完美地拟合了数据；

- 第三个模型完美拟合了所有训练数据，但却不能很好地拟合真实的函数，也就是对于不同的训练数据很敏感（高方差）。

对于这两个问题，我们可以选择模型和超参数来得到效果更好的配置，也就是可以通过验证曲线调节。

- 验证曲线与学习曲线。验证曲线和学习曲线的区别是：

  - 横轴为某个超参数的一系列值，由此来看不同参数设置下模型的准确率，而不是不同训练集大小下的准确率。

  - 从验证曲线上可以看到随着超参数设置的改变，模型可能从欠拟合到合适再到过拟合的过程，进而选择一个合适的设置，来提高模型的性能。

  - 需要注意的是如果我们使用验证分数来优化超参数，那么该验证分数是有偏差的，它无法再代表模型的泛化能力，我们就需要使用其他测试集来重新评估模型的泛化能力。

  - 不过有时画出单个超参数与训练分数和验证分数的关系图，有助于观察该模型在相应的超参数取值时，是否有过拟合或欠拟合的情况发生。

- 使用学习曲线判别偏差和方差问题

  - 如果一个模型相对于训练集来说过于复杂，比如参数太多，则模型很可能过拟合。避免过拟合的手段包含增大训练集，但这是不容易做到的。通过画出不同训练集大小对应的训练集和验证集准确率，我们能够很轻松滴检测模型是否方差偏高或偏差过高，以及增大训练集是否有用。

![验证曲线——2](https://raw.githubusercontent.com/6studentsfromsspku/sspku-300-concepts/master/lcx引用图片/验证曲线_2.png)
- 上图的左上角子图中模型偏差很高。它的训练集和验证集准确率都很低，很可能是欠拟合。解决欠拟合的方法就是增加模型参数，比如，构建更多的特征，减小正则项。

- 上图右上角子图中模型方差很高，表现就是训练集和验证集准确率相差太多。解决过拟合的方法有增大训练集或者降低模型复杂度，比如增大正则项，或者通过特征选择减少特征数。

这俩问题可以通过验证曲线解决。
我们先看看学习曲线是怎么回事吧：

```
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.cross_validation import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
import matplotlib.pyplot as plt
from sklearn.learning_curve import learning_curve
import numpy as np
from sklearn.learning_curve import validation_curve
#导入数据
df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data',header=None)
X=df.loc[:,2:].values
y=df.loc[:,1].values
le=LabelEncoder()
y=le.fit_transform(y)#类标整数化
print (le.transform(['M','B']))
#划分训练集合测试集
X_train,X_test,y_train,y_test = train_test_split (X,y,test_size=0.20,random_state=1)
#标准化、模型训练串联
pipe_lr=Pipeline([('scl',StandardScaler()),('clf',LogisticRegression(random_state=1,penalty='l2'))])


case1：学习曲线
构建学习曲线评估器，train_sizes：控制用于生成学习曲线的样本的绝对或相对数量
train_sizes,train_scores,test_scores=learning_curve(estimator=pipe_lr,X=X_train,y=y_train,train_sizes=np.linspace(0.1,1.0,10),cv=10,n_jobs=1)
统计结果
train_mean= np.mean(train_scores,axis=1)
train_std = np.std(train_scores,axis=1)
test_mean =np.mean(test_scores,axis=1)
test_std=np.std(test_scores,axis=1)
绘制效果
plt.plot(train_sizes,train_mean,color='blue',marker='o',markersize=5,label='training accuracy')
plt.fill_between(train_sizes,train_mean+train_std,train_mean-train_std,alpha=0.15,color='blue')
plt.plot(train_sizes,test_mean,color='green',linestyle='--',marker='s',markersize=5,label='test accuracy')
plt.fill_between(train_sizes,test_mean+test_std,test_mean-test_std,alpha=0.15,color='green')
plt.grid()
plt.xlabel('Number of training samples')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')
plt.ylim([0.8,1.0])
plt.show()
```
![验证曲线——3](https://raw.githubusercontent.com/6studentsfromsspku/sspku-300-concepts/master/lcx引用图片/验证曲线_3.jpg)
- 用验证曲线解决过拟合和欠拟合。
  - 验证曲线是非常有用的工具，他可以用来提高模型的性能，原因是他能处理过拟合和欠拟合问题。

  - 验证曲线和学习曲线很相近，不同的是这里画出的是不同参数下模型的准确率而不是不同训练集大小下的准确率：


```
case2：验证曲线
param_range=[0.001,0.01,0.1,1.0,10.0,100.0]
10折，验证正则化参数C
train_scores,test_scores =validation_curve(estimator=pipe_lr,X=X_train,y=y_train,param_name='clf__C',param_range=param_range,cv=10)
统计结果
train_mean= np.mean(train_scores,axis=1)
train_std = np.std(train_scores,axis=1)
test_mean =np.mean(test_scores,axis=1)
test_std=np.std(test_scores,axis=1)
plt.plot(param_range,train_mean,color='blue',marker='o',markersize=5,label='training accuracy')
plt.fill_between(param_range,train_mean+train_std,train_mean-train_std,alpha=0.15,color='blue')
plt.plot(param_range,test_mean,color='green',linestyle='--',marker='s',markersize=5,label='test accuracy')
plt.fill_between(param_range,test_mean+test_std,test_mean-test_std,alpha=0.15,color='green')
plt.grid()
plt.xscale('log')
plt.xlabel('Parameter C')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')
plt.ylim([0.8,1.0])
plt.show()
```
![验证曲线——4](https://raw.githubusercontent.com/6studentsfromsspku/sspku-300-concepts/master/lcx引用图片/验证曲线_4.jpg)

- 我们得到了参数C的验证曲线。

- 和learningcurve方法很像，validationcurve方法使用采样k折交叉验证来评估模型的性能。在validation_curve内部，我们设定了用来评估的参数，这里是C,也就是LR的正则系数的倒数。

- 观察上图，最好的C值是0.1。

####2.损失函数
by lcx
- 概述：
  - 通常机器学习每一个算法中都会有一个目标函数，算法的求解过程是通过对这个目标函数优化的过程。在分类或者回归问题中，通常使用损失函数（代价函数）作为其目标函数。损失函数用来评价模型的预测值和真实值不一样的程度，损失函数越好，通常模型的性能越好。不同的算法使用的损失函数不一样。

  - 损失函数分为经验风险损失函数和结构风险损失函数。经验风险损失函数指预测结果和实际结果的差别，结构风险损失函数是指经验风险损失函数加上正则项。通常表示为如下：



- 常见的损失函数
  - 0-1损失函数和绝对值损失函数。0-1损失是指，预测值和目标值不相等为1，否则为0：
![](https://raw.githubusercontent.com/6studentsfromsspku/sspku-300-concepts/master/lcx引用图片/损失函数_1.png)

- 感知机就是用的这种损失函数。但是由于相等这个条件太过严格，因此我们可以放宽条件，即满足时认为相等。

  - ![](https://raw.githubusercontent.com/6studentsfromsspku/sspku-300-concepts/master/lcx引用图片/损失函数_3.png)
- 绝对值损失函数为：
  - ![](https://raw.githubusercontent.com/6studentsfromsspku/sspku-300-concepts/master/lcx引用图片/损失函数_4.png)

- log对数损失函数：

  - 逻辑斯特回归的损失函数就是对数损失函数，在逻辑斯特回归的推导中，它假设样本服从伯努利分布（0-1）分布，然后求得满足该分布的似然函数，接着用对数求极值。逻辑斯特回归并没有求对数似然函数的最大值，而是把极大化当做一个思想，进而推导它的风险函数为最小化的负的似然函数。从损失函数的角度上，它就成为了log损失函数。

- log损失函数的标准形式：
  - ![](https://raw.githubusercontent.com/6studentsfromsspku/sspku-300-concepts/master/lcx引用图片/损失函数_5.png)

在极大似然估计中，通常都是先取对数再求导，再找极值点，这样做是方便计算极大似然估计。损失函数L（Y，P（Y|X））是指样本X在分类Y的情况下，使概率P(Y|X)达到最大值（利用已知的样本分布，找到最大概率导致这种分布的参数值）

- 平方损失函数

最小二乘法是线性回归的一种方法，它将回归的问题转化为了凸优化的问题。最小二乘法的基本原则是：最优拟合曲线应该使得所有点到回归直线的距离和最小。通常用欧几里得距离进行距离的度量。平方损失的损失函数为：

![](https://raw.githubusercontent.com/6studentsfromsspku/sspku-300-concepts/master/lcx引用图片/损失函数_6.png)

- 指数损失函数

  - AdaBoost就是一指数损失函数为损失函数的。   指数损失函数的标准形式：![](https://raw.githubusercontent.com/6studentsfromsspku/sspku-300-concepts/master/lcx引用图片/损失函数_7.png)

- Hinge损失函数。Hinge loss用于最大间隔（maximum-margin）分类，其中最有代表性的就是支持向量机SVM。Hinge函数的标准形式：
![](https://raw.githubusercontent.com/6studentsfromsspku/sspku-300-concepts/master/lcx引用图片/损失函数_8.png)　　（与上面统一的形式：![](https://raw.githubusercontent.com/6studentsfromsspku/sspku-300-concepts/master/lcx引用图片/损失函数_9.png) ）其中，t为目标值（-1或+1），y是分类器输出的预测值，并不直接是类标签。其含义为，当t和y的符号相同时（表示y预测正确）并且|y|≥1时，hinge loss为0；当t和y的符号相反时，hinge loss随着y的增大线性增大。
- 在支持向量机中，最初的SVM优化的函数如下：
  - ![](https://raw.githubusercontent.com/6studentsfromsspku/sspku-300-concepts/master/lcx引用图片/损失函数_10.png)

- 将约束项进行变形，则为：![](https://raw.githubusercontent.com/6studentsfromsspku/sspku-300-concepts/master/lcx引用图片/损失函数_11.png)

- 则损失函数可以进一步写为：
![](https://raw.githubusercontent.com/6studentsfromsspku/sspku-300-concepts/master/lcx引用图片/损失函数_12.png)

因此，SVM的损失函数可以看做是L2正则化与Hinge loss之和。

几种损失函数的曲线如下图：
![](https://raw.githubusercontent.com/6studentsfromsspku/sspku-300-concepts/master/lcx引用图片/损失函数_13.png)
参考文献：[常见的损失函数](https://www.cnblogs.com/hejunlin1992/p/8158933.html)
