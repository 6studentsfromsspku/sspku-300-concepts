
####文档写作统一规范：

1，空格： 用两个&#8195; 半空格，用&#8198;

2，标题：用###

3，行内公公式使用两个\$,行间公式使用四个$。公式处，必须使用公式符号，其他地方，若非引起歧义，尽可能少的使用公式符号。

4，图片引用格式：![图片编号，如9_1](图片网址)
中括号中写入图片的标题。当图片加载不出时，会显示中括号内的内容。


[TOC]


####机器学习的300个概念

翻译：
sspku：lcx、wdw、wgw、wqq、zcj、zzx
tutor：wgr

####1.Adaboost:
Adaboost是一种通过迭代弱分类器而产生最终的强分类器的算法，其步骤如下：
####2.Avoid Overfiting:
####3,Hamming Loss
####4,handing imbalanced
by-wgw
![4](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-4.jpg?raw=true)
什么是不平衡数据：
不平衡对于分类问题来说是指数据集中样本的类别不平均。

比如， 对于一个样本总数为100的二分类问题来说，80个样本被标为类别1，剩下的20个样本被标为类别2.这是一个不平衡的数据集，因为类别1比类别2的样本总数为4:1.

不仅仅是二分类问题有类别不平衡问题，多分类问题也存在类别不平衡的问题。

不平衡问题很常见
大部分的分类数据集中各类别的样本总数不会绝对一样，但是稍稍有一些差别无妨。

比如，描述欺诈性交易的数据集是不平衡的。绝大多数的交易记录是“正常”，只有很小一部分是“欺诈”。

另一个例子是顾客流失（customer churn）数据集，绝大多数的顾客不会流失，只有一小部分顾客会取消订阅（“流失”）。

当类别不平衡的的比例比较大时，比如接近4:1，不平衡就会导致机器学习算法出问题。

把所有的结果都归为一类
当模型训练不平衡数据集的时候会发生什么？

就如你可能会猜想的那样，在不平衡数据集上我们仍然可能会得到很高的准确率。若90%的样本是类别1，那么模型的准确率就会接近90%。因为先验概率决定一个样本被标为类别1的概率是90%，所以模型就会总是将样本标为类别1并得到高的准确率。

那么在支持向量机模型中怎么处理数据不平衡的问题呢？
一个可用的方法是引入代价敏感因子，设计出代价敏感的分类算法。通常对小样本赋予较高的代价，大样本赋予较小的代价，期望以此来平衡样本之间的数目差异。
比如在支持向量机中，调整超参数C，
即$$C_k=C*(1-w_k)$$
其中$w_k$是数据集中k类别的数量比例。这样，k类别的数量比例越大，其惩罚参数越小，k类别的数量比例越小，其惩罚参数越大，可以使模型有效的平衡不平衡数据集带来的影响。





参考资料：
https://blog.csdn.net/lime1991/article/details/47952505
####5,handling outliers
####6,hessian matrix
####7,heteroskedasticity
####8.Hidden Layer
-by zcj
![8](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-8.jpg?raw=true)

&#8195;&#8195;类神经网路（neural network），简单来说想要模拟人类大脑神经的方式来达到复杂的学习效果 。一个简单的神经网路模型如下：
![Alt text](./Hidden Layer_神经网路模型.jpeg)

&#8195;&#8195;神经网路是由许许多多的神经元（neuron）所组成，也就是图中那些圆形．每个神经元或多或少会跟其他的神经元有所连结， 在机器学习里面会把神经元归纳在以下三个layer里面：
&#8195;&#8195;1、Input Layer：里面每个神经元可以想像成某笔训练资料的所有特征，以图中例子资料会有x1跟x2两个特征。
&#8195;&#8195;2、Output Layer：某笔训练资料对应的输出结果，若是二元分类问题或是回归问题通常在Output层只会有一个神经元，而在做多元分类的时候就会有多个神经元当作输出。
&#8195;&#8195;3、Hidden Layer:非Input Layer跟Output Layer的中间层一律归类在Hidden Layer(不是资料的输入也不是资料的输出，所以名为Hidden Layer。每一个隐藏层可以对上一层的输出进行非线性变换, 因此深度神经网络拥有比“浅层”网络拥有更加优异的表达能力)
Input Layer跟Output Layer只会有一层，而Hidden Layer根据设计可以有很多层，而除了Output Layer以外，会在每一层加上一个bias的神经元当作一个常数，这样一来每神经元的运算就能变成机器学习里面常用的数学公式：
$$z=w_1x_1+w_2x_2+···+w_nx_n+bias$$
&#8195;&#8195;至于为什么需要神经网路，什么样的情况下适用神经网路，一般来说就是当训练资料我们没有办法定义其特征的时候，就可以试着使用神经网路。一些Raw data，像是图片，音波等这些binary档案。因为神经网路的Hidden Layer代表着一层又一层的特征抽取过程，当Hidden Layer很多且设计的还不错的时候，有可能就可以对于那些Raw data抽取出更有物理意义的特征来做学习。这也是一些特征难以定义的应用像是图形辨识、语音辨识、NLP等等喜欢用神经网路当作当作学习的模型的原因。

参考资料：http://terrence.logdown.com/posts/1132631-neural-networks-with-backpropagation-one-notes
####9.Hinge-Loss
-by wgw
&#8195;&#8195;合页损失函数，是支持向量机学习的损失函数，也是支持向量机目标函数的一部分。支持向量机是一种常见的分类器，其一般模型为找出一个超平面$w^*·x+b^*=0$使用决策函数$f(x)=sign(w^*·x+b^*)$对数据集进行分类。其数学表示为最小化以下目标函数：
$$\sum_{i=1}^N[1-y_i(w·x_i+b)]_++\lambda·||w||^2$$

其中$[1-y_i(w·x_i+b)]_+$下角标$+$代表取正，即当$[]$内值为负时取为0，非负时取原值。相当于$max\{[1-y_i(w·x_i+b)],0\}$,其中$w·x_i+b$代表支持向量机的输出,如果我们以$w$代表这个输出值，以$y$
表示真实值，则有$max\{[1-y·w,0\}$，那么合页损失函数就是:
$$L_{Hinge}(w,y)=max\{[1-y·w,0\}$$
其中$y$取值范围为$\{1,-1\}$,也就是y的真实分类结果，$w$的取值范围为$(-\infty,+\infty)$,我们希望分类器的输出结果$w$能够与$y$一致,但是这是很难的，一般情况下，会出现以下几种情况，这几种情况下损失函数的输出结果代表了，我们对分类情况的评价，使得分类器可以不断的学习进步。
$1$，$w$与$y$同号，但是$w$的绝对值小于$1$，这时$max\{[1-y·w,0\}$的取值为一个正数，只是这个正数相对于$4$中分类错误的情况给出的损失值不大，是一个大于$0$但是小于$1$的正数，代表这种情况是有一定微小损失的。
$2$，$w$与$y$同号，并且$w$的绝对值等于$1$，即分类完全正确，这时$max\{[1-y·w,0\}$的取值为零，代表这种情况是没有损失的。
$3$，$w$与$y$同号，但是$w$的绝对值大于$1$，这时$max\{[1-y·w,0\}$的取值为0，代表这种情况是没有损失的。但是同时这种情况的得分与$2$中是完全一样的，即合页损失函数并不鼓励分类器过度自信。
$4$，$w$与$y$异号，不管$w$的绝对值大于还是小于$1$，这时$max\{[1-y·w,0\}$的取值均为大于$1$的正数，对于这种严重的分类错误，合叶损失函数给出较大的损失值也是符合逻辑的。
合页损失函数图像：
![9_1](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/wgw%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/9_1.jpeg?raw=true)
合页损失函数的函数图像因状似合页而得名。

####10.How &#8198Norm &#8198Penalties&#8198Work :
&#8195&#8195Norm Penalties 范数惩罚，是一种减少参数的值的不一致性的方法。所谓范数类似于“长度”：
L2范数为$||X||_2=\sqrt{\sum_{i=1}^n{x_i^2}}$是欧式距离。
L1范数为$||X||_1=\sum_{i=1}^n|{x_i}|$
L0范数为$||X||_0=\sum_{i=1}^n1({x_i\neq0})$,也就是参数中不为零的元素个数。
通过L2范数，我们可以使参数尽可能的接近于零，
L1范数有很多的名字，例如我们熟悉的曼哈顿距离、最小绝对误差等。使用L1范数可以度量两个向量间的差异，L2也可以度量两个向量间的差异，如平方差和。由于L0范数的意义很难明确，所以一般需要优化L0范数的时候都会放宽到L1或者L2.
####11.how to choose Hiddenunit activation functins
####12.hyperparameter tuning
####13.back prop

####14.Hyperplane(超平面)
![14](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-14.jpg?raw=true)

&#8195;&#8195;一個n-1維的超平面可以分開一個n維的空間。
&#8195;&#8195;舉例：一維超平面（直線）可以分開一個二維空間，二維超平面可以分開一個三維空間。

&#8195;&#8195;應用：故事是这样子的，在很久以前的情人节，大侠要去救他的爱人，但魔鬼和他玩了一个游戏。魔鬼在桌子上似乎有规律放了两种颜色的球，说：“你用一根棍分开它们？要求：尽量在放更多球之后，仍然适用。”
![14_1](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/zcj%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/14_1.jpeg?raw=true)
&#8195;&#8195;于是大侠这样放，干的不错？
![14_2](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/zcj%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/14_2.jpeg?raw=true)
&#8195;&#8195;然后魔鬼，又在桌上放了更多的球，似乎有一个球站错了阵营。
![14_3](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/zcj%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/14_3.jpeg?raw=true)
&#8195;&#8195;SVM就是试图把棍放在最佳位置，好让在棍的两边有尽可能大的间隙。
![14_4](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/zcj%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/14_4.jpeg?raw=true)
&#8195;&#8195;现在即使魔鬼放了更多的球，棍仍然是一个好的分界线。
![14_5](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/zcj%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/14_5.jpeg?raw=true)
然后，在SVM 工具箱中有另一个更加重要的 trick。 魔鬼看到大侠已经学会了一个trick，于是魔鬼给了大侠一个新的挑战。
![14_6](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/zcj%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/14_6.jpeg?raw=true)
现在，大侠没有棍可以很好帮他分开两种球了，现在怎么办呢？当然像所有武侠片中一样大侠桌子一拍，球飞到空中。然后，凭借大侠的轻功，大侠抓起一张纸，插到了两种球的中间。
![14_7](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/zcj%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/14_7.jpeg?raw=true)
现在，从魔鬼的角度看这些球，这些球看起来像是被一条曲线分开了。
![14_8](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/zcj%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/14_8.jpeg?raw=true)
再之后，无聊的大人们，把这些球叫做 「data」，把棍子叫做 「classifier」, 最大间隙trick叫做「optimization」， 拍桌子叫做「kernelling」, 那张纸叫做「hyperplane」。
参考资料：https://blog.csdn.net/sinat_35512245/article/details/54981721


####15.hypothesis space
####16.iid（独立同分布）：
by-wgw
![16](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-16.jpg?raw=true)
独立同分布：
相互独立并且服从同一分布。
独立的：
每一次观察都是一次独立的事件。随机变量X1和X2独立,是指X1的取值不影响X2的取值,X2的取值也不影响X1的取值.
同一分布：
每一次的观察都是从同一种概率分布集中的抽取。
随机变量X1和X2同分布,意味着X1和X2具有相同的分布形状和相同的分布参数,对离散随机变量具有相同的分布律,对连续随机变量具有相同的概率密度函数,有着相同的分布函数,相同的期望、方差。
说明：独立同分布是机器学习中的一个重要的假设，机器学习模型在训练集中训练出的模型如何能够在测试集中有效，就是基于训练集和测试集的数据是独立同分布的。
关于独立同分布，西瓜书这样解释道：
输入空间中的所有样本服从一个隐含未知的分布，训练数据所有样本都是独立地从这个分布上采样而得。
那为什么非要有这个假设呢？
我们知道，机器学习就是利用当前获取到的信息（或数据）进行训练学习，用以对未来的数据进行预测、模拟。所以都是建立在历史数据之上，采用模型去拟合未来的数据。因此需要我们使用的历史数据具有总体的代表性。

为什么要有总体代表性？我们要从已有的数据（经验） 中总结出规律来对未知数据做决策，如果获取训练数据是不具有总体代表性的，就是特例的情况，那规律就会总结得不好或是错误，因为这些规律是由个例推算的，不具有推广的效果。

通过独立同分布的假设，就可以大大减小训练样本中个例的情形。

机器学习并不总是要求数据同分布。在不少问题中要求样本（数据）采样自同一个分布是因为希望用训练数据集训练得到的模型可以合理用于测试集，使用同分布假设能够使得这个做法解释得通。

由于现在的机器学习方向的内容已经变得比较广，存在不少机器学习问题并不要求样本同分布，比如一些发表在机器学习方向上的online算法就对数据分布没啥要求，关心的性质也非泛化性。


参考资料：
https://blog.csdn.net/frbevrqbn4l/article/details/79372973
####17.imputation
####18.imputing missing values
####19.inflestion point
####20.initialization of neural net parameters
####21.initiali zing weights
####22.instrumental variables
by-wgw
![22](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-22.jpg?raw=true)

通俗一点说，回归的思想就是先抓住x，然后观察y将如何变化。比如说居民收入r与消费c，先抓住1000元收入水平的消费群体，然后看他们将如何消费，c|1000是条件随机变量（当然，实际数据中1000元水平的观测可能只有一个）；然后再抓住1500元收入水平的群体，再看他们将如何消费，依次类推。一般来说，随着收入增长，消费的条件均值将同步增长，此时回归关系成立。

但是，令我们苦恼的是，实际中很有可能是“无法抓住x”的，因为x在变，y也在变，然后y的变化又影响到了x，所以我们观测到的结果，很有可能是x与y相互影响的结果；通俗一点说，就是x已经与y纠缠到了一起，你哪里还能辨清哪是x，哪是y？比如说收入与消费，可以说赚得多，花得也多，但钱花完了，又得想办法去多赚点，这时收入与消费是相互影响的，你是无法"按住x"的。因为等你"按住x"了，去观察y，y的变动回过头来又造成了x的变化，你转身一看，坏了，x已经不是原来那个x了，它已经变了！这个相互影响的过程，你是观测不到的，你观测到的只是结果。所以在你观测到实际数据的时候，x已经不是本来的x，x中混杂了y的信息。既然x已经不是本来意义上的x，你又如何去估计它对Y的真实影响？这就是我们通常所说的联立性偏误（simultaneity bias），即x与y是同时变动的。这种情况下，x与回归模型的误差项表现为相关，违背了经典OLS(ordinary least square,普通最小二乘法)的假设。此时，你应该可以知道，你很难估计x对y的真实影响，即在经典回归假设下，估计出的回归系数是有偏的。这是造成内生性 Endogeneity 的情况之一。

* 内生性的原因
内生性的根源：互为因果、联立性、遗漏变量、测量误差
总的说来，内生性主要由以下原因造成：
1. 遗漏变量：如果遗漏的变量与其他解释变量不相关，一般不会造成问题。否则，就会造成解释变量与残差项相关，从而引起内生性问题。
2. 解释变量与被解释变量相互影响
3. 度量误差 （measurement error）：由于关键变量的度量上存在误差，使其与真实值之间存在偏差，这种偏差可能会成为回归误差（regression error）的一部分，从而导致内生性问题。
* 内生性的例子
工资和受教育水平同时受到能力的影响，然而，即使我们可以通过其他相关的测试得出能力的代理变量，能力是不可直接观测的变量，这就带来了遗漏变量的内生性问题。又比如，在联立方程中，消费和收入同时受一些宏观因素的影响，这就带来了联立方程偏误。还有卡片中的例子，政策与抗议相关，但是又是相互影响的，你很难分清楚哪个是因哪个是果，这就是解释变量与被解释变量相互影响的问题。我们可以通过工具变量的方法来解决内生性的问题。工具变量要与解释变量相关，且与被解释变量不相关，这样就可以观察工具变量来预测被解释变量了。



参考资料：http://bbs.pinggu.org/thread-5987081-1-1.html
####23.interaction term
####24.Bag-of-Words
-by zcj
![24](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-24.jpg?raw=true)
&#8195;&#8195;Bag-of-words模型是信息检索领域常用的文档表示方法。在信息检索中，BoW模型假定对于一个文档，忽略它的单词顺序和语法、句法等要素，将其仅仅看作是若干个词汇的集合，文档中每个单词的出现都是独立的，不依赖于其它单词是否出现。也就是说，文档中任意一个位置出现的任何单词，都不受该文档语意影响而独立选择的。例如有如下两个文档：

1：Bob likes to play basketball, Jim likes too.
2：Bob also likes to play football games.

基于这两个文本文档，构造一个词典:
Dictionary={1:'Bob',2:'like',3:'to',4:'play',5:'basketball',6:'also',7:'football',8:'games',9:'Jim',10:'to'}

&#8195;&#8195;这个词典一共包含10个不同的单词，利用词典的索引号，上面两个文档每一个都可以用一个10维向量表示（用整数数字0-n表示某个单词在文档中出现的次数）：

1：[1, 2, 1, 1, 1, 0, 0, 0, 1, 1]
2：[1, 1, 1, 1 ,0, 1, 1, 1, 0, 0]

&#8195;&#8195;从上述的表示中，可以很清楚地看出来，在文档表示过程中并没有考虑关键词的顺序，而是仅仅将文档看成是一些关键词出现的概率的集合(这是Bag-of-Words模型的缺点之一)，每个关键词之间是相互独立的，这样每个文档可以表示成关键词出现频率的统计集合，类似于直方图的统计表示。

参考资料：http://blog.csdn.net/wsj998689aa/article/details/47089153

####25.intercept term
####26.interpolation
####27.iqr interquartile range
####28.issues with platt scaline
![28](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-28.jpg?raw=true)
####29.
####30.
####31.
####32.
####33.
####34.K-Nearest Neighbors（K近邻法）：
by-wgw
![34](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-34.jpg?raw=true)
k近邻法(k-nearest neighbor, k-NN)是1967年由Cover T和Hart P提出的一种基本分类与回归方法。它的工作原理是：存在一个样本数据集合，也称作为训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一个数据与所属分类的对应关系。输入没有标签的新数据后，将新的数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本最相似数据(最近邻)的分类标签。一般来说，我们只选择样本数据集中前k个最相似的数据，这就是k-近邻算法中k的出处，通常k是不大于20的整数。最后，选择k个最相似数据中出现次数最多的分类，作为新数据的分类。
举个简单的例子，我们可以使用k-近邻算法分类一个电影是爱情片还是动作片。
![34_1](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/wgw%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/34_1.png?raw=true)

上表就是我们已有的数据集合，也就是训练样本集。这个数据集有两个特征，即打斗镜头数和接吻镜头数。除此之外，我们也知道每个电影的所属类型，即分类标签。用肉眼粗略地观察，接吻镜头多的，是爱情片。打斗镜头多的，是动作片。以我们多年的看片经验，这个分类还算合理。如果现在给我一部电影，你告诉我这个电影打斗镜头数和接吻镜头数。不告诉我这个电影类型，我可以根据你给我的信息进行判断，这个电影是属于爱情片还是动作片。而k-近邻算法也可以像我们人一样做到这一点，不同的地方在于，我们的经验更"牛逼"，而k-近邻算法是靠已有的数据。比如，你告诉我这个电影打斗镜头数为2，接吻镜头数为102，我的经验会告诉你这个是爱情片，k-近邻算法也会告诉你这个是爱情片。你又告诉我另一个电影打斗镜头数为49，接吻镜头数为51，我"邪恶"的经验可能会告诉你，这有可能是个"爱情动作片"，画面太美，我不敢想象。 (如果说，你不知道"爱情动作片"是什么？请评论留言与我联系，我需要你这样像我一样纯洁的朋友。) 但是k-近邻算法不会告诉你这些，因为在它的眼里，电影类型只有爱情片和动作片，它会提取样本集中特征最相似数据(最邻近)的分类标签，得到的结果可能是爱情片，也可能是动作片，但绝不会是"爱情动作片"。当然，这些取决于数据集的大小以及最近邻的判断标准等因素。

2、距离度量
我们已经知道k-近邻算法根据特征比较，然后提取样本集中特征最相似数据(最邻近)的分类标签。那么，如何进行比较呢？比如，我们还是以表1.1为例，怎么判断红色圆点标记的电影所属的类别呢？ 如下图所示。
![34_1](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/wgw%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/34_2.png?raw=true)

我们可以从散点图大致推断，这个红色圆点标记的电影可能属于动作片，因为距离已知的那两个动作片的圆点更近。k-近邻算法用什么方法进行判断呢？没错，就是距离度量。这个电影分类的例子有2个特征，也就是在2维实数向量空间，可以使用我们高中学过的两点距离公式计算距离，如下：$$|AB|=\sqrt{(x_1-x_2)^2+(y_1-y_2)^2} $$
通过计算，我们可以得到如下结果：

(101,20)->动作片(108,5)的距离约为16.55
(101,20)->动作片(115,8)的距离约为18.44
(101,20)->爱情片(5,89)的距离约为118.22
(101,20)->爱情片(1,101)的距离约为128.69
通过计算可知，红色圆点标记的电影到动作片 (108,5)的距离最近，为16.55。如果算法直接根据这个结果，判断该红色圆点标记的电影为动作片，这个算法就是最近邻算法，而非k-近邻算法。那么k-近邻算法是什么呢？k-近邻算法步骤如下：

计算已知类别数据集中的点与当前点之间的距离；
按照距离递增次序排序；
选取与当前点距离最小的k个点；
确定前k个点所在类别的出现频率；
返回前k个点所出现频率最高的类别作为当前点的预测分类。
比如，现在我这个k值取3，那么在电影例子中，按距离依次排序的三个点分别是动作片(108,5)、动作片(115,8)、爱情片(5,89)。在这三个点中，动作片出现的频率为三分之二，爱情片出现的频率为三分之一，所以该红色圆点标记的电影为动作片。这个判别过程就是k-近邻算法。
参考资料：http://cuijiahua.com/blog/2017/11/ml_1_knn.html








####35.
####36.
####36.KNN  Neighborhood  Size（KNN邻域尺寸）:
by-wgw
![36](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-36.jpg?raw=true)
&#8195;&#8195;KNN邻域尺寸，指的就是K的取值大小。中国有句古话，近朱者赤，近墨者黑，孔子曰：德不孤必有邻，都是说相近的多少总是有些相似的。K近邻法就是基于这样的思想，以一个点的最近邻的K个点的特征来预测这个点的特征，那么当K越大时，所包围进来的点就越多，那么准确性就会越差，也即是偏差越大，但是方差会越小。反过来当K越小时，所包围进来的点就越少，那么准确性就会越高，也即是偏差越小，但是方差会变大。
####37.
####38.
####39.
####40.
####41.
####42.
####43.
####44.
####45.
####46.Bagging
by-zzx
![46](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-46.jpg?raw=true)
含义:Bootstrap Aggregating（Bagging）是常用的统计学习方法，此算法的核心是将目标训练数据集进行N次Bootstrap采样得到N个训练数据子集，对每个子使用相同的算法分别建立决策树，最终的分类、回归结果是N个决策树的结果多数投票或平均（对于回归问题可以取平均值，对于分类问采多数投票）。

描述 ： 算法不直接作用于模型本身，而是作用在训练数据上。
原有的训练集，如图中的“Data”，通过有放回的随机抽样方法，（Bootstrap采样），生成N个子训练集，“Datastrapped Data”系列 。对每个子集使用相同的算法分别建立决策树，最终的分类或回归结果是 N个决策树的结果的多数投票或平均。

Bootstrap采样：
1）采用重抽样技术从原始本中取一定数量（自己给）的，此过程允许重复抽样。
2）根据抽出的样本计算给定的统计量T。
3）重复上述N次（一般大于1000），得到N个统计量T。
4）计算上述N个统计量得到统计量的方差。
特点：Bagging较单棵决策树来说，降低了方差；但由于将多棵决策树的结果进行了平均，损失了模型的可解释性。
![46_1](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/zzx%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/46_1.jpeg?raw=true)
参考资料：
http://blog.csdn.net/foolsnowman/article/details/51726007
http://blog.csdn.net/u010158659/article/details/51248877
####47.
####48.
####49.
####50.
####51.
####52.Log-Sum-Exp（指数函数的和的对数）：
by-wgw
![52](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-52.jpg?raw=true)
假设我们有N个实数quicklatex.png，我们想求下式：
$$z=log\sum_{n=1}^N{exp\{x_n\}}$$


这是非常常见的需求，比如你想用softmax去计算一个多项式分布（多分类逻辑回归）。你要计算对数似然的话，你就得计算上式这种规范化因子。如果quicklatex (2).png很大或很小，朴素的直接计算会上溢出或下溢出，从而导致严重问题。举个例子，对于quicklatex (3).png，直接计算是可行的，我们可以得到1.55。但对于quicklatex (4).png，却并不可行，我们会得到quicklatex (6).png；对于quicklatex (7).png，还是不行，我们会得到quicklatex (9).png。这是怎么回事？很简单，你的浮点数只有64位，在计算指数函数的环节，quicklatex (10).png，会发生上溢出；quicklatex (11).png，会发生下溢出。即便在数学世界上式的值显然不是无穷大，但在计算机的浮点数世界里就是求不出来。怎么办呢？

解决方案很简单：

$$log\sum_{n=1}^N{exp\{x_n\}}=a+log\sum_{n=1}^N{exp\{x_n-a\}}$$



对任意a都成立，这意味着我们可以自由地调节指数函数的指数部分，一个典型的做法是取$\{x_n\}_{n=1}^N$中的最大值：
$$a={max} \; x_n$$


这可以保证指数最大不会超过0，于是你就不会上溢出。即便剩余的部分下溢出了，你也能得到一个合理的值。
推导过程：
![52_1](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/wgw%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/52_1.png?raw=true)
参考资料：http://www.hankcs.com/ml/computing-log-sum-exp.html



####53.




####54.
####55.

####56.Manhattan Distance（曼哈顿距离）
by-zcj
![56](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-56.jpg?raw=true)
![56_1](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/zcj%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/56_1.jpeg?raw=true)
&#8195;&#8195;图中红线代表曼哈顿距离，绿色代表欧氏距离，也就是直线距离，而蓝色和黄色代表等价的曼哈顿距离。曼哈顿距离——两点在南北方向上的距离加上在东西方向上的距离，即$d(i,j)=|x_i-x_j|+|y_i-y_j|$。对于一个具有正南正北、正东正西方向规则布局的城镇街道，从一点到达另一点的距离正是在南北方向上旅行的距离加上在东西方向上旅行的距离，因此，曼哈顿距离又称为出租车距离。
&#8195;&#8195;應用：向 OLS 公式中分別添加一个系数惩罚项， 一個為歐式距離的懲罰項，另一個為曼哈頓距離。
![56_2](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/zcj%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/56_2.jpeg?raw=trueg)
&#8195;&#8195;这就是系数平方和的惩罚项与绝对值和的惩罚项之间的区别。在左图中，随着 λ 变化以及最小点的移动，平方惩罚项产生的切点一般不会落在坐标轴上。Β1 与 β2 都不为 0。相比之下，在右图中，绝对值和惩罚项产生的切点落在了 β2 的轴上。在 β2 轴上，β1=0。一个稀疏的系数向量相当于算法告诉你可以忽略一些因变量。


####57.
####58.Matrices（矩阵）：
by-wgw
![58](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-58.jpg?raw=true)
由 m × n 个数$a_{ij}$排成的m行n列的数表称为m行n列的矩阵，简称m × n矩阵。记作：
$$
        \begin{pmatrix}
        a_{11} & a_{12} & \cdots & a_{1n} \\
        a_{21} & a_{22} & \cdots & a_{2n} \\
        a_{31} & a_{32} & \cdots & a_{3n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & \cdots & a_{mn} \\
        \end{pmatrix}
$$
这m×n 个数称为矩阵A的元素，简称为元，数$a_{ij}$位于矩阵A的第i行第j列，称为矩阵A的(i,j)元，以数$a_{ij}$为(i,j)元的矩阵可记为($a_{ij}$)或($a_{ij}$)m × n，m×n矩阵$A$也记作$A_{mn}$。元素是实数的矩阵称为实矩阵，元素是复数的矩阵称为复矩阵。而行数与列数都等于n的矩阵称为n阶矩阵或n阶方阵。

####59.
####60.
####61.
####62.
####63.
####64.MSE（均方误差）：
by-wgw
![64](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-64.jpg?raw=true)
MSE: Mean Squared Error
均方误差是指参数估计值与参数真值之差平方的期望值;
MSE可以评价数据的变化程度，MSE的值越小，说明预测模型描述实验数据具有越好的精确度。
$$MSE=\frac{1}{N}\sum_{t=1}^{N}(observed_t-predict_t)^2$$
####65.
####66.
####67.
####68.
####69.
####70.Min Max Scaling
by-wgw
![70](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-70.jpg?raw=true)

对房屋售价进行预测时，我们的特征仅有房屋面积一项，但是，在实际生活中，卧室数目也一定程度上影响了房屋售价。下面，我们有这样一组训练样本：
房屋面积（英尺） | 卧室数量（间） | 售价（美元）
- | :-: | -:
2104 | 3 | 399900
1600 | 3 | 329900
2400 | 3 | 369000
1416 | 2 | 232000
3000 | 4 | 539900
1985 | 4 | 299900
.... | ... | ....


注意到，房屋面积及卧室数量两个特征在数值上差异巨大，如果直接将该样本送入训练，则代价函数的轮廓会是“扁长的”，在找到最优解前，梯度下降的过程不仅是曲折的，也是非常耗时的：
![70_1](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/wgw%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/70_1.png?raw=true)


该问题的出现是因为我们没有同等程度的看待各个特征，即我们没有将各个特征量化到统一的区间。
数据标准化（归一化）处理是数据挖掘的一项基础工作，不同评价指标往往具有不同的量纲和量纲单位，这样的情况会影响到数据分析的结果，为了消除指标之间的量纲影响，需要进行数据标准化处理，以解决数据指标之间的可比性。原始数据经过数据标准化处理后，各指标处于同一数量级，适合进行综合对比评价。以下是常用的归一化方法：

Min-Max Scaling
Min-Max Scaling又称为Min-Max normalization， 特征量化的公式为：
$$z=\frac{x_i-min(x_i)}{max(x_i)-min(x_i)}$$
量化后的特征将分布在区间[0,1]。
大多数机器学习算法中，会选择Standardization来进行特征缩放，但是，Min-Max Scaling也并非会被弃置一地。在数字图像处理中，像素强度通常就会被量化到[0,1]区间，在一般的神经网络算法中，也会要求特征被量化[0，1]区间。
进行了特征缩放以后，代价函数的轮廓会是“偏圆”的，梯度下降过程更加笔直，收敛更快性能因此也得到提升：
![70_2](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/wgw%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/70_2.png?raw=true)



参考资料：https://blog.csdn.net/leiting_imecas/article/details/54986045

####71.
####72.
####73.
####74.
####75.
####76.Model selection
by-wgw
![76](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-76.jpg?raw=true)












####77.
####78.
####79.
####80.
####81.
####82.
####83.
####84.
####85.
####86.
####87.
####88.Normalized Initialization of Neural Net Parameters
![88](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-88.jpg?raw=true)
####89.
####90.
####91.Notation 1（符号）:
![91](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-91.jpg?raw=true)
符号：
﹁： “非”运算，如命题p，﹁p为其非命题。
$∋$：使得，又记成“s.t”。
$a \in b$： a属于b，a属于集合b，a是集合b的元素。
$\therefore$ ：所以，用于逻辑书写等。
####92.
####93.
####94.
####95.
####96.
####97.
####98.
####99.
####100.One Hot encoding（独热编码）:

&#8195;&#8195;问题由来：在很多機器學習任务中，特征并不总是连续值，而有可能是分类值。
&#8195;&#8195;例如，考虑以下的三个特征：
["male", "female"]
["from Europe", "from US", "from Asia"]
["male", "from US", v"uses Internet Explorer"] 表示为[0, 1, 3]
["female", "from Asia", "uses Chrome"]表示为[1, 2, 1]
&#8195;&#8195;但是，即使转化为数字表示后，上述数据也不能直接用在我们的分类器中。因为，分类器往往默认数据是连续的（可以计算距离），并且是有序的（而上面这个0并不是说比1要高级）。但是，按照我们上述的表示，数字并不是有序的，而是随机分配的。
&#8195;&#8195;为了解决上述问题，其中一种可能的解决方法是采用独热编码（One-Hot Encoding）。独热编码即 One-Hot 编码，又称一位有效编码，其方法是使用N位状态寄存器来对N个状态进行编码，每个状态都由他独立的寄存器位，并且在任意时候，其中只有一位有效。
&#8195;&#8195;例如：
自然状态码为：000,001,010,011,100,101；
独热编码为：000001,000010,000100,001000,010000,100000；
&#8195;&#8195;可以这样理解，对于每一个特征，如果它有m个可能值，那么经过独热编码后，就变成了m个二元特征（如成绩这个特征有好，中，差变成one-hot就是100, 010, 001）。并且，这些特征互斥，每次只有一个激活。因此，数据会变成稀疏的。
&#8195;&#8195; 这样做的好处主要有：
&#8195;&#8195; 1.解决了分类器不好处理属性数据的问题。2.在一定程度上也起到了扩充特征的作用。
举例：
基于Scikit-learn的例子：
```
from sklearn import preprocessing
enc = preprocessing.OneHotEncoder()
enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])
enc.transform([[0, 1, 3]]).toarray()
```
输出结果：
```
array([[ 1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.]])
```
注意: fit了4个数据3个特征，而transform了1个数据3个特征。第一个特征两种值(0: 10, 1: 01)，第二个特征三种值(0: 100, 1: 010, 2: 001)，第三个特征四种值(0: 1000, 1: 0100, 2: 0010, 3: 0001)。所以转换[0, 1, 3]为[ 1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.]。
參考資料：https://blog.csdn.net/pipisorry/article/details/61193868

####101.
####102.
####103.
####104.
####105.Out-Of-Bag  Error（袋外损失）:
by-wgw
![105](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-105.jpg?raw=true)
&#8195;&#8195;Out-Of-Bag Error也称为OOB损失或者袋外损失。在随机森林模型中袋外损失起到了测试集测试的功能。袋外损失是泛化损失的无偏估计。在用Bootstrap方法构建随机森林时，会将数据集随机抽样出三分之二用于构建一棵决策树。剩余的三分之一可以起到测试集的作用，但是这里与测试集有所区别，一棵树的测试集可能是另一棵树的训练集。Breiman发现，用袋外数据得出的损失，是泛化的误差的无偏估计，也就是说，袋外损失可以起到表征泛化误差的功能。
####106.Out-Of-Core（外存算法）：
![106](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-106.jpg?raw=true)
当数据集太大以至于不能完全存储与计算机的内存中，就要使用外存算法了。
外存的选择：
分块预处理数据
分行读取或预处理数据
渐进式学习
随机性算法
部分拟合学习方法

####107.
####108.Overfit vs Underfit（过拟合与欠拟合）:
by-zcj
![108](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-108.jpg?raw=true)
&#8195;&#8195;过拟合：简单理解就是训练样本的得到的输出和期望输出基本一致，但是测试样本输出和测试样本的期望输出相差却很大。为了得到一致假设而使假设变得过度复杂称为过拟合。想像某种学习算法产生了一个过拟合的分类器，这个分类器能够百分之百的正确分类样本数据（即再拿样本中的文档来给它，它绝对不会分错），但也就为了能够对样本完全正确的分类，使得它的构造如此精细复杂，规则如此严格，以至于任何与样本数据稍有不同的文档它全都认为不属于这个类别！
&#8195;&#8195;欠拟合：如果数据本身呈现二次型，故用一条二次曲线拟合会更好。但普通的PLS程序只提供线性方程供拟合之用。这就产生拟合不足即“欠拟合”现象，从而在预报时要造成偏差。如果我们用人工神经网络拟合，则因为三层人工神经网络拟合能力极强，有能力拟合任何函数。如果拟合彻底，就会连实验数据点分布不均匀，实验数据的误差等等“噪声”都按最小二乘判据拟合进数学模型。这当然也会造成预报的偏差。这就是“过拟合”的一个实例了。
参考资料：https://blog.csdn.net/cindysuna/article/details/50057579

####109.
####110.
####111.
####112.调整R平方
by-wgw
![112](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-112.jpg?raw=true)
我们先看看什么是R方：
$$R^2=1-\frac{RSS}{TSS}$$
其中RSS是残差，$RSS=\sum(y_i-\hat{y})^2$是模型的残差。$TSS=\sum(y_i-\overline{y})^2$是模型的总方差。RSS/TSS 可以认为是模型难以解释的部分，$R^2=1-\frac{RSS}{TSS}$则可以认为是模型可以解释的部分，也就是模型的可靠度，显然R方越大，模型也越被认为是可靠的。
但是，在其他变量不变的情况下，引入新的变量，总能提高模型的R方。但是在机器学习中，直觉告诉我们：当全部的正确的特征都被加入后，再引入新的特征，应该对新的特征进行惩罚。于是就有了调整R方，或者说修正R方：
$$R^2=1-\frac{RSS/(n-d-1)}{TSS/(n-1)}$$
其中n是观察的数量，d是特征的数量，这样，当特征增多，R方有降低的压力，也即是有了惩罚措施。使得算法可以剔除不必须的特征，这也是符合“奥卡姆剃刀”原理的。


####113.Bias（偏差）:
by-wgw
![113](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-113.jpg?raw=true)
偏差，是模型与真实值之间的期望误差。$$ Bias[\hat{f}(x)]=E[\hat{f}(x)-f(x)]$$其中$\hat{f}(x)$是模型的预测值。$f(x)$是真实值。预测值与真实值的期望误差，就是偏差。
####114.
####115.

####116.Partial Derivative（偏导数）：
-by zcj
![116](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-116.jpg?raw=true)

&#8195;&#8195;在数学中，一个多变量的函数的偏导数，就是保持其他变量恒定下关于其中一个变量的导数而（相对于全导数，在其中所有变量都允许变化）。偏导数在向量分析和微分几何中用途广泛。

1.引入
&#8195;&#8195;在一元函数中，导数就是函数的变化率。对于二元函数研究它的“变化率”，由于自变量多了一个，情况就要复杂的多。在 xOy 平面内，当动点由$P(x_0,y_0)$沿不同方向变化时，函数 f(x,y) 的变化快慢一般说来是不同的，因此就需要研究 f(x,y) 在$P(x_0,y_0)$点处沿不同方向的变化率。
在这里我们只学习函数 f(x,y) 沿着平行于 x 轴和平行于 y 轴两个特殊方位变动时， f(x,y) 的变化率。
偏导数的表示符号为:∂。
偏导数反映的是函数沿坐标轴正方向的变化率。
2.定义
&#8195;&#8195;x方向的偏导：设有二元函数 z=f(x,y) ，点$(x_0,y_0)$是其定义域D内一点。把y固定在$y_0$而让x在$x_0$ 有增量$\Delta_x$ ，相应地函数 z=f(x,y) 有增量（称为对 x 的偏增量）
$$\Delta_z=f(x_0+\Delta_x,y_0)-f(x_0,y_0)$$
&#8195;&#8195;如果$\Delta_z$与$\Delta_x$之比当 $\Delta_x\to0$ 时的极限存在，那么此极限值称为函数 z=f(x,y)在$(x_0,y_0)$处对 x 的偏导数，记作 $f^\prime_ x(x_0,y_0)$。函数 z=f(x,y)在$(x_0,y_0)$处对x的偏导数，实际上就是把y固定在$y_0$看成常数后，一元函数$z=f(x,y_0)$在$x_0$处的导数。
&#8195;&#8195;y方向的偏导：同样，把x固定在$x_0$，让y有增量$\Delta_y$，如果极限存在那么此极限称为函数 z=(x,y) 在 $(x_0,y_0)$处对y的偏导数。记作
$f^\prime_y(x0,y0)$。
3.求法
&#8195;&#8195;当函数 z=f(x,y) 在$(x_0,y_0)$的两个偏导数$ f^\prime_x(x_0,y_0)$与$ f^\prime_y(x_0,y_0)$都存在时，我们称 f(x,y) 在$(x_0,y_0)$处可导。如果函数 f(x,y) 在域D的每一点均可导，那么称函数 f(x,y) 在域D可导。此时，对应于域 D 的每一点 (x,y) ，必有一个对 x (对 y )的偏导数，因而在域 D 确定了一个新的二元函数，称为 f(x,y) 对 x (对 y )的偏导函数。简称偏导数。按偏导数的定义，将多元函数关于一个自变量求偏导数时，就将其余的自变量看成常数，此时他的求导方法与一元函数导数的求法是一样的。
4.几何意义
&#8195;&#8195;表示固定面上一点的切线斜率。偏导数$ f^\prime_x(x_0,y_0)$表示固定面上一点对 x 轴的切线斜率；偏导数$ f^\prime_y(x_0,y_0)$表示固定面上一点对 y 轴的切线斜率。
高阶偏导数：如果二元函数 z=f(x,y) 的偏导数$ f^\prime_x(x_0,y_0)$与$ f^\prime_y(x_0,y_0)$ 仍然可导，那么这两个偏导函数的偏导数称为 z=f(x,y) 的二阶偏导数。二元函数的二阶偏导数有四个：$f^{\prime\prime}_{xx},f^{\prime\prime}_{xy},f^{\prime\prime}_{yx},f^{\prime\prime}_{yy}$。
&#8195;&#8195;注意：$f^{\prime\prime}_{xy}$ 与 $f^{\prime\prime}_{yx}$的区别在于：前者是先对 x 求偏导，然后将所得的偏导函数再对 y 求偏导；后者是先对 y 求偏导再对 x 求偏导。当$f^{\prime\prime}_{xy}$ 与 $f^{\prime\prime}_{yx}$都连续时，求导的结果与先后次序无关。

补充：
如何理解导数的概念 ? https://www.zhihu.com/question/28684811
参考资料：
https://baike.baidu.com/item/%E5%81%8F%E5%AF%BC%E6%95%B0/5536984?fr=aladdin
####117.
####118.

####119.perceptron（感知机）：
by-zcj
![119](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-119.jpg?raw=true)
&#8195;&#8195;在机器学习中，感知机（Perceptron）是二分类的线性分类模型，属于监督学习算法。输入为实例的特征向量，输出为实例的类别（取+1和-1）。感知机对应于输入空间中将实例划分为两类的分离超平面。感知机旨在求出该超平面，为求得超平面导入了基于误分类的损失函数，利用梯度下降法对损失函数进行最优化。感知机的学习算法具有简单而易于实现的优点，分为原始形式和对偶形式。感知机预测是用学习得到的感知机模型对新的实例进行预测的，因此属于判别模型。感知机由Rosenblatt于1957年提出的，是神经网络和支持向量机的基础。
定义：假设输入空间(特征向量)为$X \subseteq R_n$，输出空间为Y={-1, +1}。输入$ x\in X$表示实例的特征向量，对应于输入空间的点；输出y∈Y表示示例的类别。由输入空间到输出空间的函数为$f(x)=sign(w·x+b)$称为感知机。其中，参数w为权值向量weight，b称为偏置bias。w·x表示w和x的点积。sign为符号函数，即
$$ \sum_{i=1}^{m}w_ix_i=w_1x_1+w_2x_2+···+w_n x_n$$
一种线性分类模型，属于判别模型。我们需要做的就是找到一个最佳的满足w⋅x+b=0的w和b值，即分离超平面（separating hyperplane）。
学习策略
核心：极小化损失函数。如果训练集是可分的，感知机的学习目的是求得一个能将训练集正实例点和负实例点完全分开的分离超平面。为了找到这样一个平面（或超平面），即确定感知机模型参数w和b，我们采用的是损失函数，同时将损失函数极小化。对于损失函数的选择，我们采用的是误分类点到超平面的距离（可以自己推算一下，这里采用的是几何间距，就是点到直线的距离）：
$$ \frac{1}{||w||}|w·x_0+b |$$
其中||w||是L2范数。
对于误分类点$(x_i,y_i)$来说：$$-y_i(w·x_i+b)>0$$
误分类点到超平面的距离为：$$ -\frac{1}{||w||}y_i(w·x_0+b)$$
那么，所有点到超平面的总距离为：$$ -\frac{1}{||w||}\sum_{x_i \in M}y_i(w·x_0+b)$$
不考虑$\frac{1}{ ||w|| }$，就得到感知机的损失函数：$$L(w,b)= -\sum_{x_i \in M}y_i(w·x_0+b)$$
其中M为误分类的集合。这个损失函数就是感知机学习的经验风险函数。可以看出，损失函数L(w,b)是非负的。如果没有误分类点，则损失函数的值为0，而且误分类点越少，误分类点距超平面的距离之和就越小，损失函数值就越小。同时，损失函数L(w,b)是连续可导函数。

学习算法：
感知机学习转变成求解损失函数L(w,b)的最优化问题。最优化的方法是随机梯度下降法（stochastic gradient descent），这里采用的就是该方法。说明一下，梯度下降其实是局部最优。感知机学习算法本身是误分类驱动的，因此我们采用随机梯度下降法。首先，任选一个超平面w0和b0，然后使用梯度下降法不断地极小化目标函数。极小化过程不是一次使M中所有误分类点的梯度下降，而是一次随机的选取一个误分类点使其梯度下降。
参考资料：https://blog.csdn.net/dream_angel_z/article/details/48915561

####120.
####121.
####122.
####123.
####124.大O符号
edit by wgw
![124](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-124.jpg?raw=true)
大O符号代表数学上的趋近。这个符号有两种形式上很接近但迥然不同的使用方法：无穷大渐近与无穷小渐近。然而这个区别只是在运用中的而不是原则上的——除了对函数自变量的一些不同的限定，“大O”的形式定义在两种情况下都是相同的。
无穷大渐近:
大O符号在分析算法效率的时候非常有用。举个例子，解决一个规模为 n 的问题所花费的时间（或者所需步骤的数目）可以被求得：$T(n) = 4n^2 - 2n + 2$。
当 n 增大时，$n^2$项将开始占主导地位，而其他各项可以被忽略——举例说明：当 n = 500，$4n^2$项是 2n 项的1000倍大，因此在大多数场合下，省略后者对表达式的值的影响将是可以忽略不计的。
进一步看，如果我们与任一其他级的表达式比较，$n^2$项的系数也是无关紧要的。例如一个包含$n^3$或$n^2$项的表达式，即使$T(n) = 1,000,000n^2$假定$U(n) = n^3$一旦 n 增长到大于1,000,000，后者就会一直超越前者$(T(1,000,000) = 1,000,000^3 = U(1,000,000))$。
这样，大O符号就记下剩余的部分，写作：
$T(n)∈O(n^2)$
并且我们就说该算法具有 2阶的时间复杂度。
无穷小渐近:
大O也可以用来描述数学函数估计中的误差项。例如：
$e^x=1+x+x^2/2+O(x^3)$当 x→0 时
这表示，如果 x 足够接近于0，那么误差$e^x-(1+x+x^2/2)$的绝对值小于 $x^3$的某一常数倍。

常用的符号和阶：
|符号|阶|
|---|---|
|O(1)|常数|
|O(log*n)|迭代对数|
|O(log n)|对数|
|O[(log n)^c]|多对数|
|O(n)|线性，次线性|
|O(n log n)|线性对数，或对数线性、拟线性、超线性|
|O(n^2)|平方|
|O(n^c),Integer(c>1)|多项式，有时叫作“代数”（阶）|
|O(c^n)|指数，有时叫作“几何”（阶）|
|O(n!)|阶乘，有时叫做“组合”（阶）|
####125.
####126.
####127.
####128.
####129.
####130.
####131.
####132.
####133.
####134.
####135.
####136.Recall（召回率）：
edit by wgw
![136](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-136.jpg?raw=true)
召回率：
Recall直译为回忆率，一开始是表示搜索引擎能够回忆起多少跟关键词相关的内容，那也就是搜索到的内容中跟关键词相关的内容占全部内容的比例。机器学习中的召回率也是如此，一个分类器分类为正的数据占全部为正的数据的比例，表征的是分类器将将正的数据挑出来的能力。如果我们想要找到所有正的样本，我们可以最大化召回率这个指标。



####137.
####138.
####139.
####140.
by-zzx
![140](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-140.jpg?raw=true)
激活函数：
  Activation Function，将“激活的神经元的特征”通过函数把特征保留并映射出来，是神经网络中能解决非线性问题的关键。使用线性方程，即使拓展到多层的神经网络，只是生成更复杂的线性方程，仍是线性分类。使用激励函数，再经历神经网络的叠加后，输出非线性函数，增强了神经网络的表达能力。

常见的激励函数有tanh,sigmoid,ReLU等。
  tanh　　   双切正切函数，取值范围[-1,1]
  sigmoid　 采用S形函数，取值范围[0,1]
  ReLU         大于0的留下，其他一律为0
ReLU:
  Rectified Linear Unit，线性整流函数，通常指斜坡函数，即
    f(x) = max(0,x)
优点：
  使用 ReLU得到的SGD的收敛速度会比 sigmoid/tanh 快。这是因为它是linear，而且ReLU只需要一个阈值就可以得到激活值，不用去计算复杂的运算。
  缺点：
  训练过程该函数不适应较大梯度输入，因为在参数更新以后，ReLU的神经元不会再有激活的功能，导致梯度永远都是零。
拓展:
Leaky ReLUs
    f(x) = αx (x<0)
         = x  (x≥0)
  α 是一个很小的常数，这样既修正了数据分布，又保留了一些负轴的值，使得负轴信息不会全部丢失。
Parametric ReLU
    f(x) = max(0,x)+α×min(0,x)
  α是可学习参数。为固定的非零较小数时，它等价于LeakyReLU；当它为0时，PReLU等价于ReLU。
Randomized ReLU
  Randomized Leaky ReLU 是 Leaky ReLU 的随机版本。在训练过程中，α是从一个高斯分布U(l,u)中随机出来的，然后再测试过程中进行修正。

  如下是一次ReLU层的参数
     message ReLUParameter {
     optional float negative_slope = 1 [default = 0]; //如之前分析的，默认值0即为ReLU，非零则为LeakyReLU
    enum Engine {
      DEFAULT = 0;
      CAFFE = 1;
      CUDNN = 2;
    }
    optional Engine engine = 2 [default = DEFAULT]; //运算引擎选择，一般选择默认
    }

####141.
####142.Ridge Regression（岭回归）:
by-wgw
![142](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-142.jpg?raw=true)
&#8195;&#8195;岭回归，是对最小二乘法，也就是一般线性回归的优化，通过增加惩罚项，虽然损失了一点精确性，但是模型的稳定性更好，泛化能力更好了。
普通的线性回归会遇到一些问题，主要表现在：
假设样本的数量是n，特征的数量是p，
当n>>p时，最小二乘回归会有较小的方差，
当n～p时，容易产生过拟合，
当n<p时，最小二乘回归得不到有意义的结果。
岭回归就是在平方误差的基础上增加L2正则项：
$$ RSS+\lambda\sum_{j=1}^{p}\hat{B}_j^2$$
其中$\lambda$是调节参数，通过确定的值可以使得在方差和偏差之间达到平衡：随着$\lambda$的增大，模型方差减小而偏差增大。

####143.
####144.
####145.
####146.
####147.
####148.
####149.
####150.
####151.
####152.
####153.
####154.
####155.

####156.Softmax Normalization:
by-zcj
![](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-156.jpg?raw=true)
&#8195;&#8195;Softmax函数的本质就是将一个K 维的任意实数向量压缩（映射）成另一个K维的实数向量，其中向量中的每个元素取值都介于（0，1）之间。
$$\sigma:R^k \rightarrow (0,1)^K$$
$$\sigma(z)_j=\frac{e^{z_j}}{\sum_{k=1}^{K}e^{z_k}} for j=1,...,K.$$
![156_1](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/zcj%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/156_1.jpeg?raw=true)

&#8195;&#8195;举例：如果我们输入[1，2，3，4，1，2，3]，那么softmax是[0.024，0.064，0.175，0.475，0.024，0.064，0.175]。输出的大部分重量都是原始输入中的'4'。这是该功能通常用于：突出显示最大值并抑制显着低于最大值的值。但是请注意：softmax不是尺度不变的，所以如果输入[0.1,0.2,0.3,0.4,0.1,0.2,0.3]（其总和为1.6），softmax将是[0.125,0.138,0.1513,0.169,0.125， 0.138，0.153]。这表明，对于0到1之间的值，softmax事实上不重视最大值（注意，0.169不仅小于0.475，它也小于初始值0.4）。
&#8195;&#8195;Softmax标准化是一种减少数据中极端值或异常值的影响的方法，无需从数据集中移除它们。给出离群数据非常有用，我们希望将其包含在数据集中，同时仍将数据的重要性保留在均值的标准差内。
参考资料：
https://en.wikipedia.org/wiki/Softmax_function#Softmax_normalization
https://blog.csdn.net/u014422406/article/details/52805924
https://www.zhihu.com/question/23765351
####157.Brier Score（布莱尔分数）:
by-wgw
![157](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-157.jpg?raw=true)
&#8195;&#8195;布莱尔分数，是观测值的预测概率与实际结果直接的差的平方的平均值。
$$BS=\frac{1}{n}\sum_{t=1}^{n}(P_t-O_t)^2$$
其中$P_t$是预测概率，$O_t$是实际输出。
布莱尔分数在0到1之间。平方保证了结果为正值，布莱尔分数越低说明模型的预测效果越好。
####158.
####159.
####160.
####161.
####162.
####163.
####164.
####165.
####166.
####167.
####168.
####169.
####170.
####171.
####172.$AdjustedR^2$ ( 调整$R^2$ ) :
-by zcj
![172](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-112.jpg?raw=true)
&#8195;&#8195;当因变数Y 与自变数 X 之间的关系可以用一个回归方程式来解释时，X的解释能力有必要进一步地了解。该解释能力的程度大小，即回归分析的配合度(goodness of  fit)，是以决定系数(R2)来描述。如图一所示，总变异$(Y-\overline{Y})$是由两个成分组成：配合值离平均值之变异$(\hat{Y}-\overline{Y})$，以及观测值离配合值之变异$(Y-\hat{Y})$。前者$(\hat{Y}-\overline{Y})$是由所建立回归式之X 所引起，而后者$(Y-\hat{Y})$则为逢机机差所引起。由建立回归式之 X 所引起之平方和占总变异平方和的比例，称为决定系数即$R^2=\frac{\sum(\hat{Y}-\overline{Y})^2}{\sum(Y-\hat{Y})^2}=\frac{SSR}{SST}$ ；有些教科书则将$R^2$翻译为判定系数。$R^2$的所在范围在0与1之间，其结果的大小表示Y的变异中X所能解释的程度。当$R^2$值越接近1时，表示估计式中大部份Y之变异是由X影响而来，也代表利用X来解释Y的能力越强，因此所建立的回归模式为合适可接受。
![172_1](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/zcj%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/172_1.jpeg?raw=true)
&#8195;当自变数不只一项时，例如同时探讨氮、磷、钾多种肥料对作物产量的影响关系，或气温、日照、雨量等各种气象因素对产量的综合影响，一组自变数$X_1,X_2,X_3,…$与因变数Y之间的直线关系，可以利用复回归方程式(multiple regression function)来表示：$\hat{Y}=b_0+b_1X_1+b_2X_2+b_3X_3+····$。但并非所有生物现象都是呈直线关系，当肥料用量增加时，作物产量可能会以一缓慢的速度增加，以致于该曲线会逐渐平稳而接近水平；当过度施肥时，甚至对作物造成毒害而使曲线下降。因此有时候，非直线回归方程式的探讨，是有其必要性的。对同一套试验资料而言，到底应该适合于何种特定形式的回归方程式，常常也是我们探讨的重点。而此时，$R^2$是用来作为判断回归方程式是否有效的一个重要指标。也就是说，回归系数是看个别自变数与因变数间的净关系，而决定系数则是看全部自变数与因变数间的综合关系。
&#8195;&#8195;修正的公式是:
$$R^{2}_{adj}=\frac{(n-1)(1-R^2)}{n-p-1}$$
&#8195;&#8195;其中n是样本数量，p是模型中变量的个数。
&#8195;&#8195;我们知道在其他变量不变的情况下，引入新的变量，总能提高模型的$R^2$。$R^2_{adj}$就是相当于给变量的个数加惩罚项。换句话说，如果两个模型，样本数一样，$R^2$一样，那么从$R^2_{adj}$的角度看，使用变量个数少的那个模型更优。

参考资料：
http://ilc.hk.edu.tw/c/document_library/get_file?p_l_id=260741&folderId=261080&name=DLFE-3350.pdf
http://sofasofa.io/forum_main_post.php?postid=1000702

####173.

####174.Supervised vs Unsupervised（有监督和无监督）:
![174](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-174.jpg?raw=true)
&#8195;&#8195;机器学习的类型有两种：有监督学习和无监督学习。
&#8195;&#8195;有监督学习：在有监督学习中，我们有一些训练数据。使用这些训练数据，我们创建了一个预测模型，可用于未来预测结果。最终目标是建立一个称为假设函数的预测函数f(x)。 “学习”使用数学算法对该函数进行优化，以便给定输入数据x关于某个域，它将能够预测一些有趣的值f(x)。例如，房子的市场价格。属于此类别的一些算法是回归、分类、KNN、决策树、朴素湾、支持向量机等。
&#8195;&#8195;示例：假设你提供了装满不同水果的篮子，你的任务是对它们进行分组。假设这些水果是苹果、香蕉、西瓜、橙子。既然你熟悉水果，你可以很容易地将它们分成苹果、香蕉、西瓜、橙子等各种类别。这项活动被称为训练数据。现在，如果您获得不同类似的水果，您将在未来使用此学习体验。这种类型的算法被称为分类。
&#8195;&#8195;无监督学习：与有监督学习不同，在无监督情况下，我们拥有带有输入x的训练数据集，但没有任何目标值或我们可以预测的内容。此信息缺失。这种学习的目标是在数据集内找出一组类似的例子。这种算法被称为聚类。依上例，现在让我们说，如果你再次提供了装满水果的桶，但这次你不知道那些水果。你以前从未见过他们。然后你将使用身体特征将他们放在不同的组中。比方说颜色，大小等，这称为聚类。因此，无监督学习任务涉及识别数据内的关系。这里没有提供培训示例。比如用K-means聚类算法。
参考资料：https://www.linkedin.com/pulse/machine-learning-supervised-vs-unsupervised-amit-sinha
####175.
####176.
####177.
####178.
####179.
####180.
####181.
####182.
####183.
####184.
####185.

####186.Tht Effect Of Feature Scaling On Gradient Descent:
by-wgw
![186](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-186.jpg?raw=true)
&#8195;&#8195;特征缩放在梯度下降中的效果，在梯度下降算法中，存在这样一个问题，当不同的特征的范围差距过大时，模型的预测会变得困难很多，代价函数的轮廓图会非常的偏斜，下降最快的方向与全局下降最快的方向往往不一致，直观图如下所示：
![186_1](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/wgw%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/186_1.jpeg?raw=true)

经过特征缩放之后，代价函数的轮廓图就会变的相对标准，下降最快的方向与全局下降最快的方向几乎一致：
![186_2](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/wgw%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/186_2.jpeg?raw=true)
特征缩放的一个最简答的方法就是将特征映射到0-1之间：
$$x^{,}=\frac{x-x_{min}}{x_{max}-x_{min}}$$
将每一个特征都代入就可以实现特征的一致化，也就是特征的缩放。

####187.

####188.The Random In Random Forest（随机森林中的随机）:
by-wgw
![188](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-188.jpg?raw=true)
随机森林中的随机,体现在：
1，每一棵决策树的训练集都是在总训练集中有放回的随机抽样出的的随机样本，称为袋内数据，每一棵树的袋外数据作为这棵树的测试集。
2，每一棵树都拿到所有的特征，但是每一个节点只有这些特征的一部分子集是可用的。

####189.
####190.
####191.
####192.
####193.
####194.
####195.
####196.
####197.
####198.
####199.
####200.
####201.
####202.
####203.
####204.
####205.
####206.
####207.
####208.
####209.
####210.
####211.

####212.（特征选择中的卡方应用）：
by-wgw
![212](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-212.jpg?raw=true)
&#8195;&#8195;在数据预处理的过程中，重要的一步是特征选择，也就是选择出与模型和算法最相互适应的特征。特征选择的方法有好几种，卡方检验是其中一种，卡方检验的做法是对特征与目标进行关联给出卡方值，卡方值从小到大排序，越靠前的特征效果越好。卡方检验由英国统计学家Pearson在1900年提出。
通用的卡方检验步骤：
1）假设命题$H_0$和$H_1$，$H_0$命题表示不符合假设的分布，$H_1$命题表示符合假设的分布。
2）计算卡方值，根据以下公式：$$\chi^2=\sum_1^n\frac{(O_i-E_i)}{E_i}$$对于两列数据，$O_i$表示实际值，$E_i$表示理论值，所谓理论值即是按照特定分布应取的期望值。$n-1$为自由度。
3）查卡方检验表：
![212_1](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/wgw%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/212_1.jpeg?raw=true)

4）计算出的$\chi^2$值如果小于查出的值，则拒绝$H_0$接受$H_1$，否则拒绝$H_1$接受$H_0$
####213.
####214.
####215.
####216.
####217.

####218.Why Is It Called A Cost Function（成本函数）:
by-wgw
![218](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-218.jpg?raw=true)
&#8195;&#8195;成本函数，是用一个模型代替真实世界的“成本”。这里借用了经济学里面的成本概念。成本函数可以认为是模型值与真实值之间的差异的度量。成本函数是我们要尽量最小化的。之所以叫做成本函数，是因为用模型代替世界产生的差异的确就像是我们要承担的成本一样。

####219.
####220.
####221.
####222.

####223.Agglometrative Clustering:
![223](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-223.jpg?raw=true)
&#8195;&#8195;Agglometrative Clustering,简写为AGNES，是一种自下而上的聚类算法，是层次聚类的两种方法的其中一种，另一种采用的是自上而下的分拆方法。AGNES的方法是将数据集中的点按照距离分类，这里的距离有很多种，最简单的可以是欧氏距离。AGNES聚类的步骤如下：
第一步，将所有的数据点单独为类，也就是一万个数据点那就有一万个类，
第二步，不断的合并类，这里合并的依据是类与类之间的距离。
第三步，直到某一个指标达到，则停止。如果不停止，那么所有的数据点都会聚为同一个类。
这里比较值得注意的点是，第二步里，类与类之间的距离的确定。
一般有三种方法确定三种不同定义的距离：
对于类$C_i$与$C_j$,$x_i$属于$C_i$,$x_j$属于$C_j$

1最小距离：类与类之间的距离最小的数据点的距离作为类之间的距离。
数学表示为：$$ d_{min}=(C_i,C_j)=min   distance(x_i,x_j) $$

2最大距离：类与类之间的距离最大的数据点的距离作为类之间的距离。数学表示为：
$$d_{max}=(C_i,C_j)=max    distance(x_i,x_j)$$
3平均距离：类与类的所有数据点的平均距离作为类之间的距离。数学表示为：$$d_{avg}=(C_i,C_j)=\frac{1}{|C_i||C_j|}\sum_{x\in C_i} \sum_{x\in C_j}distance(x_i,x_j)$$
一般情况下，最小距离和最大距离法都由于比较“偏激”而被较少使用，使用最多的是平均距离。
用图形形象的表示层次聚类法如下：
![223_1](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/wgw%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/223_1.png?raw=true)

在最下层，每一个数据单独为类，分类距离越大，则多个类会被划分到同一个类中去。

####224.Chi-Squared（卡方）：
by-zzx
![224](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-224.jpg?raw=true)
* 卡方统计量：Chi-Square Statistic，是配合度检验的统计量，它是由各项实际观测次数（ f0 ）与理论分布次数（f1  ）之差的平方除以理论次数，然后再求和而得出的。$x^2=\sum\frac{(f_0-f_e)^2}{f_e}$ 理论次数越大，该分布与卡方分布越接近，当理论次数  时，与卡方分布符合较好。当超过20%的理论次数小于5，或至少有一个理论次数小于1时，公式右边的表达式与卡方分布偏离较大。因此，其应用条件为至少有80%的理论次数不小于5，并且每个理论次数都不小于1。
* 卡方分布：若k个随机变量Z1、……、Zk 相互独立，且数学期望为0、方差为 1(即服从标准正态分布)，则随机变量X：$X=\sum_{n=1}^{k} Z_n^2$
  被称为服从自由度为 k 的卡方分布，记作
  $X$~ $\chi^2$
  下表罗列了一些有关卡方分布的性质:
![224_1](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/zzx%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/224_1.jpeg?raw=true)
* 卡方检验
  该检验的基本思想是：首先假设H0成立，基于此前提计算出χ2值，它表示观察值与理论值之间的偏离程度。根据χ2分布及自由度可以确定在H0假设成立的情况下获得当前统计量及更极端情况的概率P。如果P值很小，说明观察值与理论值偏离程度太大，应当拒绝无效假设，表示比较资料之间有显著差异；否则就不能拒绝无效假设，尚不能认为样本所代表的实际情况和理论假设有差别。
* 利用一个例子来解释卡方检验：假设有两个分类变量X和Y，它们的值域分别为{x1, x2}和{y1, y2}，其样本频数列联表为：

|         | y1     |   y2   |  总计   |
| ---     | -----:  | :----:  | :----: |
| x1      | a   |   b     | a+b |
| x2        |   c   |   d  | c+d |
| 总计        |    a+c    |  b+d  |  a+b+c+d|
* 若要推断的论述为H0“X与Y有关系”，可以利用独立性检验来考察两个变量是否有关系，并且能较精确地给出这种判断的可靠程度。具体的做法是，由表中的数据算出$\chi^2$。$\chi^2$越大，说明“X与Y有关系”成立的可能性越小。当表中数据a，b，c，d都不小于5时，可以查阅下表来确定结论“X与Y有关系”的可信程度：

|$P(K^2>=k)$| 0.50 | 0.40 | 0.25 | 0.15 |0.10|
| ---     | -----:  | :----:  | :----: |:----: |:----: |
|k | 0.455   | 0.708 |1.323 |2.072 |2.706|

|$P(K^2>=k)$| 0.05 | 0.025 | 0010 | 0.005 | 0.001 |
| ---     | -----:  | :----:  | :----: |:----: |:----: |
| k  |3.841|5.024|6.635|7.879|10.828|
* 例如，当“X与Y有关系”的$\chi^2$为6.109，根据表格，因为5.024≤6.109<6.635，所以“X与Y有关系”成立的概率为0.025，即2.5%。

####225.
####226.
####227.
####228.
####229.
####230.
####231.
####232.
####233.
####234.
####235.
####235.AIC
--by zcj
![](https://github.com/GaoWeio/300-Concepts-of-Machine-Learning/blob/master/4_AIC.png?raw=true)

 &nbsp; 很多参数估计问题均采用似然函数作为目标函数，当训练数据足够多时，可以不断提高模型精度，但是以提高模型复杂度为代价的，同时带来一个机器学习中非常普遍的问题——过拟合。所以，模型选择问题在模型复杂度与模型对数据集描述能力（即似然函数）之间寻求最佳平衡。

$$123$$

  &#8195;&#8195;人们提出许多信息准则，通过加入模型复杂度的惩罚项来避免过拟合问题，此处我们介绍一下常用的两个模型选择方法——赤池信息准则（Akaike Information Criterion，AIC）和贝叶斯信息准则（Bayesian Information Criterion，BIC）。
&#8195;&#8195;AIC是衡量统计模型拟合优良性的一种标准，由日本统计学家赤池弘次在1974年提出，它建立在熵的概念上，提供了权衡估计模型复杂度和拟合数据优良性的标准。
&#8198;&#8198;通常情况下，AIC定义为：
$$AIC=2k-2ln(L)$$
&#8195;&#8195;其中k是模型参数个数，L是似然函数。从一组可供选择的模型中选择最佳模型时，通常选择AIC最小的模型。
&#8195;&#8195;当两个模型之间存在较大差异时，差异主要体现在似然函数项，当似然函数差异不显著时，上式第一项，即模型复杂度则起作用，从而参数个数少的模型是较好的选择。
&#8195;&#8195;一般而言，当模型复杂度提高（k增大）时，似然函数L也会增大(即RSS變小)，从而使AIC变小，但是k过大时，似然函数增速减缓，导致AIC增大，模型过于复杂容易造成过拟合现象。目标是选取AIC最小的模型，AIC不仅要提高模型拟合度（极大似然），而且引入了惩罚项，使模型参数尽可能少，有助于降低过拟合的可能性。
参考资料：http://blog.csdn.net/lynnucas/article/details/47947943

####236.
####237.
####238.
####239.
####240.
####241.
####242.
####243.
####244.

####245.Decision Boundary（决策边界）：
-by zcj
![245](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-245.jpg?raw=true)

&#8195;&#8195;决策边界就是能够把样本正确分类的一条边界，主要有线性决策边界(linear decision boundaries)和非线性决策边界(non-linear decision boundaries)。注意：决策边界是假设函数的属性，由参数决定，而不是由数据集的特征决定。下面主要举一些例子，形象化的来说明线性决策边界和非线性决策边界。先看一个线性决策边界的例子：（注：图片来源：ng的machine learning课）

![245_1](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/zcj%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/245_1.jpeg?raw=true)

&#8195&#8195再来看一个非线性决策边界的例子：

![245_2](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/zcj%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/245_2.jpeg?raw=true)
&#8195;&#8195;假设我们的数据呈现出如上图的分布情况，那么我们的模型是什么样才能适合这些数据呢？我们需要的是一个二次方特征。由不同机器学习算法可以得到不同的决策边界，不同的数据或不同的算法都会影响边界的准确性。以下图为例，紫色虚线是贝叶斯决策边界线，黑色实线则是KNN的分类边界，由图可以发现对于该组数据贝叶斯决策边界线是较理想的分类边界。
![245_3](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/zcj%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/245_3.jpeg?raw=true)
参考资料：http://blog.csdn.net/u012328159/article/details/51068427

####246.
####247.
####248.
####249.
####250.
####251.
####252.
####253.
####254.
####255.
####256.
####257.
####258.
####259.
####260.
####261.
####262.
####263.
####264.
####265.
####266.
####267.
####268.Anscombe$^\prime$s Quartet（安斯库姆四重奏）：
by-wgw
![268](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-268.jpg?raw=true)
&#8195;&#8195;安斯库姆四重奏是四组具有完全相同的统计特征的数据，这些统计特征包括均值，方差，回归曲线方程等，然而，当把这四组数据以散点图的形式展示出来时会发现这四组数据是极不相同的。安斯库姆四重奏表明，仅仅用简单的统计特征量表述数据是会遗漏某些反应了数据本质的其他重要特征的，在应用数据前对数据进行可视化是非常重要的。当然，安斯库姆四重奏是为了说明统计描述的局限性以及数据可视化的重要性，而精心设计出的四组数据，在现实中这样的情况并不多见。
安斯库姆四重奏散点图：
![268_1](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/wgw%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/268_1.jpeg?raw=true)
安斯库姆四重奏统计特征：
![268_2](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/wgw%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/268_2.jpeg?raw=true)

####269.
####270.
####271.F-Statistic（F分布）：
by-zzx
![271](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-271.jpg?raw=true)
* F分布
设X1服从自由度为m的χ2分布,X2服从自由度为n的χ2分布，且X1、X2相互独立，则称变量F=(X1/m)/(X2/n)所服从的分布为F分布，其中第一自由度为m,第二自由度为n。
其密度函数如下图所示
![271_1](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/zzx%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/271_1.jpg?raw=true)
* 有如下性质：
    1. 期望(F)=n/(n-2)，方差D(F)=2n^2(m+n-2)/m(n-2)^2(n-4)
    2. 若F\~F(m,n)，则1/F~F(n,m)
    3. 若F\~F(1,n)，T~T(n)，则F=T^2
* F统计量
  公式见卡片。其中TSS为总平方和，RSS为残差平方和，两者相减为回归平方和。
  此统计量用于F检验，用于两个及两个以上样本均数差别的显著性检验。
* F检验
  F检验，又做联合假设检验、方差比率检验、方差齐性检验。它是一种在零假设之下，统计值服从F-分布的检验。目的是通过数据分析找出对该事物有显著影响的因素，各因素之间的交互作用，以及显著影响因素的最佳水平等。是在可比较的数组中，把数据间的总的“变差”按各指定的变差来源进行分解的一种技术。
* 检验过程
  计算样本F统计量，与F表中的值比较，如果：
  F < F表 表明两组数据没有显著差异；
  F ≥ F表 表明两组数据存在显著差异。
  下表为置信度95%时的F值（单边）
![271_2](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/zzx%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/271_2.png?raw=true)
* F检验对于数据的正态性非常敏感，因此在检验方差齐性的时候，Levene检验, Bartlett检验或者Brown–Forsythe检验的稳健性都要优于F检验。

####272.F1 Score（F1分数）：
-by zcj
![272](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-272.jpg?raw=true)

&#8195;&#8195;F1分数（F1 Score），是统计学中用来衡量二分类模型精确度的一种指标。它同时兼顾了分类模型的准确率和召回率。F1分数可以看作是模型准确率和召回率的一种加权平均，它的最大值是1，最小值是0。
&#8195&#8195数学定义:F1分数（F1Score），又称平衡F分数（Balanced F Score），它被定义为精确率和召回率的调和平均数。
$$F_1=2\frac{precision·recall}{precision+recall}$$
更一般的，我们定义$F_\beta$分数为
$$F_\beta=(1+\beta^2)\frac{precision·recall}{\beta^2·precision+recall}$$

&#8195;&#8195; 物理意义：人们通常使用准确率和召回率这两个指标，来评价二分类模型的分析效果。但是当这两个指标发生冲突时，我们很难在模型之间进行比较。比如，我们有如下两个模型A、B，A模型的召回率高于B模型，但是B模型的准确率高于A模型，A和B这两个模型的综合性能，哪一个更优呢？
|   模型    |    准确率  |  召回率 |
|  :--:    |    :--:   |   :--: |
| A        |      80%  |   90%  |
| B        |      90%  |   80%  |





&#8195;&#8195;为了解决这个问题，人们提出了$F_B$分数。FB的物理意义就是将准确率和召回率这两个分值合并为一个分值，在合并的过程中，召回率的权重是准确率的B倍。F1分数认为召回率和准确率同等重要，F2分数认为召回率的重要程度是准确率的2倍，而F0.5分数认为召回率的重要程度是准确率的一半。
&#8195;&#8195;应用领域：F分数被广泛应用在信息检索领域，用来衡量检索分类和文档分类的性能。早期人们只关注F1分数，但是随着谷歌、百度等大型搜索引擎的兴起，召回率和准确率对性能影响的权重开始变得不同，人们开始更关注其中的一种，所以$F_B$分数得到越来越广泛的应用。F分数也被广泛应用在自然语言处理领域，比如命名实体识别、分词等，用来衡量算法或系统的性能。

补充：
1. G分数是另一种统一准确率和召回率的系统性能评估标准。F分数是准确率和召回率的调和平均数，G分数被定义为准确率和召回率的几何平均数。

2. 精确率、召回率、F1 值、ROC、AUC 各自的优缺点是什么?
https://www.zhihu.com/question/30643044

参考资料：https://baike.baidu.com/item/F1%E5%88%86%E6%95%B0/13864979?fr=aladdin

####273.
####274.
####275.
####276.
####277.
####278.
####279.

####280.Fowlkes-Mallows-Score（福克斯-马洛斯得分）：
by-wgw
![280](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-280.jpg?raw=true)
&#8195;&#8195;福克斯-马洛斯得分也称$FMI$得分，是对聚类模型的性能度量的一种方法。对于给定数据集，参考模型和测试模型分别给出的了簇划分。对于任意一对数据的划分，分为四种情况：
1，参考模型划入同一簇，测试模型也划入同一簇。以TP（True Positive）表示。
2，参考模型划入同一簇，测试模型未划入同一簇。以FN（False Negative）表示。
3，参考模型未划入同一簇，测试模型划入同一簇。以FP（False Positive）表示。
4，参考模型未划入同一簇，测试模型也未划入同一簇。以TN（True Negative）表示。
则FMI得分是：$$FMI=\frac{TP}{\sqrt{(TP+FN)(TP+FP)}}$$
FMI得分越高，说明测试模型与参考模型越接近。

####281.
####282.
####283.
####284.
####285.Gini-Index(基尼指数)：
by-wgw
![285](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-285.jpg?raw=true)
&#8195;&#8195;在决策树模型中，需要确定哪些分类的属性作为分枝节点。我们选择那些使得数据集每一次分类后，子集的纯度最高的属性。基尼指数就是指示数据集纯度的指标。对于第m个节点，根据属性I数据集D可以分为k类，那么基尼指数为：
$$Gini(D_i)=\sum_{k=1}^kp_{mk}(1-p_{mk})$$
&#8195;&#8195;当$p_{mk}$越接近$0$或$1$时，$Gini(D_i)$越小，也表示数据集的纯度越高，相应的属性$I$就越适合模型，当$p_{mk}$越接近$\frac{1}{2}$时，$Gini(D_i)$越大，也表示数据集$I$的纯度越低相应的属性$I$也就越不适合模型。
####286.
####287.Gradient Cliff（梯度陡峭）：
by-zzx
![287](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-287.jpg?raw=true)
梯度陡峭:
在梯度下降方法中，会选取一定的间距，沿梯度下降的方向求解极小值。当在一个间距中出现梯度陡峭现象时，可能出现预期极小值被跳过的现象，如图所示。理论上间距都趋近无穷小时，会寻找到预期的极小值，但是极大的增加了运算时间。因此需要合适的算法，保证相对较短的运算时间，并且减小避开梯度陡峭的现象。
优化方法：
Adagrad算法很好地减小了遇到上述问题的概率。它是一种自适应地为各个参数分配不同学习率的算法。
$$\theta_{i,t+1}=\theta_{i,t}-\nabla J(\theta)\eta/(G_{i,t-1}+\epsilon)^{1/2} $$
t代表每一次迭代，gt是前t步参数θi梯度的累加。η是初始学习率，之后会自动调整学习率。而ϵ是一个比较小的数，用来保证分母非0。
可以看出，随着算法不断的迭代，Gt会越来越大，整体的学习率会越来越小。所以一般来说Adagrad算法一开始是激励收敛，到了后面就慢慢变成惩罚收敛，速度越来越慢。
参考资料：
http://blog.csdn.net/luo123n/article/details/48239963
http://blog.csdn.net/tsyccnh/article/details/76769232






####288.
####289.
####290.AUC
-by zcj


![Alt text](./26_AUC Area Under The Curve.jpeg)


&#8195&#8195ROC（Receiver Operating Characteristic）曲线和AUC常被用来评价一个二值分类器（binary classifier）的优劣，对两者的简单介绍见这里。这里简单介绍ROC和AUC的特点，以及如何作出ROC曲线图并计算AUC。

ROC曲线
&#8195&#8195对于分类器（这里讨论二值分类），或者说分类算法，评价指标主要有precision、recall、F1-score、以及ROC和AUC。下图是一个ROC曲线的示例。
![Alt text](./ROC曲线示例.jpeg)
<center>ROC曲线示例</center>

&#8195&#8195正如在这个ROC曲线的示例图中看到的那样，ROC曲线的横坐标为False positive rate（FP rate），纵坐标为True positive rate（TP rate）。下图中详细说明了FP和TP是如何定义的。
![Alt text](./FP_TP定义.jpeg)


&#8195&#8195接下来考虑ROC曲线图中的四个点和一条线。第一个点(0,1)，即FPR=0, TPR=1，这意味着FN（False negative）=0，并且FP（False positive）=0。这是一个完美的分类器，它将所有的样本都正确分类。第二个点，(1,0)，即FP rate=1，TP rate=0，类似地分析可以发现这是一个最糟糕的分类器，因为它成功避开了所有的正确答案。第三个点，(0,0)，即FPR=TPR=0，即FP（False positive）=TP（True positive）=0，可以发现该分类器预测所有的样本都为负样本（Negative）。类似的，第四个点（1,1），分类器实际上预测所有的样本都为正样本。经过以上的分析，我们可以断言，ROC曲线越接近左上角，该分类器的性能越好。
&#8195&#8195下面考虑ROC曲线图中的虚线y=x上的点。这条对角线上的点其实表示的是一个采用随机猜测策略的分类器的结果，例如(0.5,0.5)，表示该分类器随机对于一半的样本猜测其为正样本，另外一半的样本为负样本。

&#8195&#8195如何画ROC曲线：对于一个特定的分类器和测试数据集，显然只能得到一个分类结果，即一组FPR和TPR结果，而要得到一个曲线，我们实际上需要一系列FPR和TPR的值，这又是如何得到的呢？我们先来看一下Wikipedia上对ROC曲线的定义：In signal detection theory, a receiver operating characteristic (ROC), or simply ROC curve, is a graphical plot which illustrates the performance of a binary classifier system as its discrimination threshold is varied.
&#8195&#8195问题在于“as its discrimination threashold is varied”。如何理解这里的“discrimination threashold”呢？我们忽略了分类器的一个重要功能“概率输出”，即表示分类器认为某个样本具有多大的概率属于正样本（或负样本）。通过更深入地了解各个分类器的内部机理，我们总能想办法得到一种概率输出。通常来说，是将一个实数范围通过某个变换映射到(0,1)区间。
    假如我们已经得到了所有样本的概率输出（属于正样本的概率），现在的问题是如何改变“discrimination threashold”呢？我们根据每个测试样本属于正样本的概率值从大到小排序。下图是一个示例，图中共有20个测试样本，“Class”一栏表示每个测试样本真正的标签（p表示正样本，n表示负样本），“Score”表示每个测试样本属于正样本的概率。

![Alt text](./AUC_概率从大到小排序.jpeg)

&#8195&#8195接下来，我们从高到低，依次将“Score”值作为阈值threshold，当测试样本属于正样本的概率大于或等于这个threshold时，我们认为它为正样本，否则为负样本。举例来说，对于图中的第4个样本，其“Score”值为0.6，那么样本1，2，3，4都被认为是正样本，因为它们的“Score”值都大于等于0.6，而其他样本则都认为是负样本。每次选取一个不同的threshold，我们就可以得到一组FP rate和TP rate，即ROC曲线上的一点。这样一来，我们一共得到了20组FP rate和TP rate的值，将它们画在ROC曲线的结果如下图：

![Alt text](./AUC_False positive rate0.jpeg)

&#8195&#8195当我们将threshold设置为1和0时，分别可以得到ROC曲线上的(0,0)和(1,1)两个点。将这些(FPR,TPR)对连接起来，就得到了ROC曲线。当threshold取值越多，ROC曲线越平滑。
&#8195&#8195AUC（Area Under Curve）被定义为ROC曲线下的面积，显然这个面积的数值不会大于1。又由于ROC曲线一般都处于y=x这条直线的上方，所以AUC的取值范围在0.5和1之间。使用AUC值作为评价标准是因为很多时候ROC曲线并不能清晰的说明哪个分类器的效果更好，而作为一个数值，对应AUC更大的分类器效果更好。
&#8195&#8195在了解了ROC曲线的构造过程后，编写代码实现并不是一件困难的事情。相比自己编写代码，有时候阅读其他人的代码收获更多，当然过程也更痛苦些。在此推荐scikit-learn中关于计算AUC的代码。
&#8195&#8195那么AUC值的含义是什么呢？根据(Fawcett, 2006)，AUC的值的含义是：
The AUC value is equivalent to the probability that a randomly chosen positive example is ranked higher than a randomly chosen negative example.
首先AUC值是一个概率值，当你随机挑选一个正样本以及一个负样本，当前的分类算法根据计算得到的Score值将这个正样本排在负样本前面的概率就是AUC值。当然，AUC值越大，当前的分类算法越有可能将正样本排在负样本前面，即能够更好的分类。

&#8195&#8195既然已经有这么多评价标准，为什么还要使用ROC和AUC呢？因为ROC曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。在实际的数据集中经常会出现类不平衡（class imbalance）现象，即负样本比正样本多很多（或者相反），而且测试数据中的正负样本的分布也可能随着时间变化。下图是ROC曲线和Precision-Recall曲线的对比：

![Alt text](./AUC_ROC曲线和P-R曲线的对比.jpeg)

&#8195&#8195在上图中，(a)和(c)为ROC曲线，(b)和(d)为Precision-Recall曲线。(a)和(b)展示的是分类其在原始测试集（正负样本分布平衡）的结果，(c)和(d)是将测试集中负样本的数量增加到原来的10倍后，分类器的结果。可以明显的看出，ROC曲线基本保持原貌，而Precision-Recall曲线则变化较大。

参考资料：http://alexkong.net/2013/06/introduction-to-auc-and-roc/

####291.
####292.
####293.
####294.
####295.
####296.
####297.
####298.
####299.
####300.
####301.


以下为曾经所做，还未并入。













####?.The Argument For Parametric Models（）
当数据生成函数大致匹配参数概率分布时，我们可以将我们的计算仅限于其参数。这使得仅使用少量信息就知道很多。 许多概率分布的灵活性意味着如果选择分布族(举例：正态跟学生T)通常不是问题 然而，范围匹配是重要的。例如，如果我们想要一个概率，我们就不应该选择输出数大于1的概率分布。

####？Variance for feature selection（特征选择中的方差）：
by-wgw
![31](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-31.jpg?raw=true)
&#8195;&#8195;特征选择中的方差，方差是特征含有的信息的一个度量，方差越大，说明特征中含有的信息越多，方差越小，说明特征中含有的信息越少，方差越小，则特征越不能很好的训练一个模型，反过来方差越大，越能很好的训练出一个模型。所以在特征选择中，我们应尽量选择那些方差较大的特征。


####?，类别特征：
&#8195;&#8195;特征有两种，一种是有序的特征，比如学习科目成绩的高低、年龄的大小……等。另一种是无序的，比如性别，水果，城市的种类。对于有序的特征，本身的数值即可作为机器处理的对象。对于无序的特征，若要转化为机器可以直接处理的对象，需要数字化，但是这里的数字化不是简单的为无序特征赋值。
无序特征的数字化：
&#8195;&#8195;对于无序特征，例如对于性别男，性别女，分别赋值1，2。那么，1小于2是否意味着男小于女？显然不是的。但是机器会认为1是小于2的，这会干扰到机器学习的效果。所以对于无序特征的数字化，应避免数字化后，使得本来无序的特征有了顺序。
&#8195;&#8195;一个可行的方法是对无序特征赋于向量值。例如对性别为男赋值$[0,1]$.对性别为女赋值$[1,0]$。若有更多的类别，可类似的赋值$[0,0,0,1]$，$[0,0,1,0]$等。
