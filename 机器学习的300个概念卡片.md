
####文档写作统一规范：

1，空格： 用两个&#8195; 半空格，用&#8198;

2，标题：用###

3，行内公公式使用两个\$,行间公式使用四个$。公式处，必须使用公式符号，其他地方，若非引起歧义，尽可能少的使用公式符号。

4，图片引用格式：![图片编号，如9_1](图片网址)
中括号中写入图片的标题。当图片加载不出时，会显示中括号内的内容。


[TOC]


####机器学习的300个概念

翻译：
sspku：lcx、wdw、wgw、wqq、zcj、zzx
tutor：wgr

####1.Adaboost:
Adaboost是一种通过迭代弱分类器而产生最终的强分类器的算法，其步骤如下：
####2.Avoid Overfiting:
####3,Hamming Loss
####4,handing imbalanced
####5,handling outliers
####6,hessian matrix
####7,heteroskedasticity
####8.Hidden Layer
-by zcj
![8](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-8.jpg?raw=true)

&#8195;&#8195;类神经网路（neural network），简单来说想要模拟人类大脑神经的方式来达到复杂的学习效果 。一个简单的神经网路模型如下：
![Alt text](./Hidden Layer_神经网路模型.jpeg)

&#8195;&#8195;神经网路是由许许多多的神经元（neuron）所组成，也就是图中那些圆形．每个神经元或多或少会跟其他的神经元有所连结， 在机器学习里面会把神经元归纳在以下三个layer里面：
&#8195;&#8195;1、Input Layer：里面每个神经元可以想像成某笔训练资料的所有特征，以图中例子资料会有x1跟x2两个特征。
&#8195;&#8195;2、Output Layer：某笔训练资料对应的输出结果，若是二元分类问题或是回归问题通常在Output层只会有一个神经元，而在做多元分类的时候就会有多个神经元当作输出。
&#8195;&#8195;3、Hidden Layer:非Input Layer跟Output Layer的中间层一律归类在Hidden Layer(不是资料的输入也不是资料的输出，所以名为Hidden Layer。每一个隐藏层可以对上一层的输出进行非线性变换, 因此深度神经网络拥有比“浅层”网络拥有更加优异的表达能力)
Input Layer跟Output Layer只会有一层，而Hidden Layer根据设计可以有很多层，而除了Output Layer以外，会在每一层加上一个bias的神经元当作一个常数，这样一来每神经元的运算就能变成机器学习里面常用的数学公式：
$$z=w_1x_1+w_2x_2+···+w_nx_n+bias$$
&#8195;&#8195;至于为什么需要神经网路，什么样的情况下适用神经网路，一般来说就是当训练资料我们没有办法定义其特征的时候，就可以试着使用神经网路。一些Raw data，像是图片，音波等这些binary档案。因为神经网路的Hidden Layer代表着一层又一层的特征抽取过程，当Hidden Layer很多且设计的还不错的时候，有可能就可以对于那些Raw data抽取出更有物理意义的特征来做学习。这也是一些特征难以定义的应用像是图形辨识、语音辨识、NLP等等喜欢用神经网路当作当作学习的模型的原因。

参考资料：http://terrence.logdown.com/posts/1132631-neural-networks-with-backpropagation-one-notes
####9.Hinge-Loss
-by wgw
&#8195;&#8195;合页损失函数，是支持向量机学习的损失函数，也是支持向量机目标函数的一部分。支持向量机是一种常见的分类器，其一般模型为找出一个超平面$w^*·x+b^*=0$使用决策函数$f(x)=sign(w^*·x+b^*)$对数据集进行分类。其数学表示为最小化以下目标函数：
$$\sum_{i=1}^N[1-y_i(w·x_i+b)]_++\lambda·||w||^2$$

其中$[1-y_i(w·x_i+b)]_+$下角标$+$代表取正，即当$[]$内值为负时取为0，非负时取原值。相当于$max\{[1-y_i(w·x_i+b)],0\}$,其中$w·x_i+b$代表支持向量机的输出,如果我们以$w$代表这个输出值，以$y$
表示真实值，则有$max\{[1-y·w,0\}$，那么合页损失函数就是:
$$L_{Hinge}(w,y)=max\{[1-y·w,0\}$$
其中$y$取值范围为$\{1,-1\}$,也就是y的真实分类结果，$w$的取值范围为$(-\infty,+\infty)$,我们希望分类器的输出结果$w$能够与$y$一致,但是这是很难的，一般情况下，会出现以下几种情况，这几种情况下损失函数的输出结果代表了，我们对分类情况的评价，使得分类器可以不断的学习进步。
$1$，$w$与$y$同号，但是$w$的绝对值小于$1$，这时$max\{[1-y·w,0\}$的取值为一个正数，只是这个正数相对于$4$中分类错误的情况给出的损失值不大，是一个大于$0$但是小于$1$的正数，代表这种情况是有一定微小损失的。
$2$，$w$与$y$同号，并且$w$的绝对值等于$1$，即分类完全正确，这时$max\{[1-y·w,0\}$的取值为零，代表这种情况是没有损失的。
$3$，$w$与$y$同号，但是$w$的绝对值大于$1$，这时$max\{[1-y·w,0\}$的取值为0，代表这种情况是没有损失的。但是同时这种情况的得分与$2$中是完全一样的，即合页损失函数并不鼓励分类器过度自信。
$4$，$w$与$y$异号，不管$w$的绝对值大于还是小于$1$，这时$max\{[1-y·w,0\}$的取值均为大于$1$的正数，对于这种严重的分类错误，合叶损失函数给出较大的损失值也是符合逻辑的。
合页损失函数图像：
![9_1](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/wgw%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/9_1.jpeg?raw=true)
合页损失函数的函数图像因状似合页而得名。

####10.How &#8198Norm &#8198Penalties&#8198Work :
&#8195&#8195Norm Penalties 范数惩罚，是一种减少参数的值的不一致性的方法。所谓范数类似于“长度”：
L2范数为$||X||_2=\sqrt{\sum_{i=1}^n{x_i^2}}$是欧式距离。
L1范数为$||X||_1=\sum_{i=1}^n|{x_i}|$
L0范数为$||X||_0=\sum_{i=1}^n1({x_i\neq0})$,也就是参数中不为零的元素个数。
通过L2范数，我们可以使参数尽可能的接近于零，
L1范数有很多的名字，例如我们熟悉的曼哈顿距离、最小绝对误差等。使用L1范数可以度量两个向量间的差异，L2也可以度量两个向量间的差异，如平方差和。由于L0范数的意义很难明确，所以一般需要优化L0范数的时候都会放宽到L1或者L2.
####11.how to choose Hiddenunit activation functins
####12.hyperparameter tuning
####13.back prop

####14.Hyperplane(超平面)
![14](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-14.jpg?raw=true)

&#8195;&#8195;一個n-1維的超平面可以分開一個n維的空間。
&#8195;&#8195;舉例：一維超平面（直線）可以分開一個二維空間，二維超平面可以分開一個三維空間。

&#8195;&#8195;應用：故事是这样子的，在很久以前的情人节，大侠要去救他的爱人，但魔鬼和他玩了一个游戏。魔鬼在桌子上似乎有规律放了两种颜色的球，说：“你用一根棍分开它们？要求：尽量在放更多球之后，仍然适用。”
![14_1](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/zcj%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/14_1.jpeg?raw=true)
&#8195;&#8195;于是大侠这样放，干的不错？
![14_2](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/zcj%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/14_2.jpeg?raw=true)
&#8195;&#8195;然后魔鬼，又在桌上放了更多的球，似乎有一个球站错了阵营。
![14_3](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/zcj%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/14_3.jpeg?raw=true)
&#8195;&#8195;SVM就是试图把棍放在最佳位置，好让在棍的两边有尽可能大的间隙。
![14_4](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/zcj%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/14_4.jpeg?raw=true)
&#8195;&#8195;现在即使魔鬼放了更多的球，棍仍然是一个好的分界线。
![14_5](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/zcj%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/14_5.jpeg?raw=true)
然后，在SVM 工具箱中有另一个更加重要的 trick。 魔鬼看到大侠已经学会了一个trick，于是魔鬼给了大侠一个新的挑战。
![14_6](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/zcj%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/14_6.jpeg?raw=true)
现在，大侠没有棍可以很好帮他分开两种球了，现在怎么办呢？当然像所有武侠片中一样大侠桌子一拍，球飞到空中。然后，凭借大侠的轻功，大侠抓起一张纸，插到了两种球的中间。
![14_7](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/zcj%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/14_7.jpeg?raw=true)
现在，从魔鬼的角度看这些球，这些球看起来像是被一条曲线分开了。
![14_8](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/zcj%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/14_8.jpeg?raw=true)
再之后，无聊的大人们，把这些球叫做 「data」，把棍子叫做 「classifier」, 最大间隙trick叫做「optimization」， 拍桌子叫做「kernelling」, 那张纸叫做「hyperplane」。
参考资料：https://blog.csdn.net/sinat_35512245/article/details/54981721


####15.hypothesis space
####16.iid
####17.imputation
####18.imputing missing values
####19.inflestion point
####20.initialization of neural net parameters
####21.initiali zing weights
####22.instrumental variables
####23.interaction term
####24.Bag-of-Words
-by zcj
![24](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-24.jpg?raw=true)
&#8195;&#8195;Bag-of-words模型是信息检索领域常用的文档表示方法。在信息检索中，BoW模型假定对于一个文档，忽略它的单词顺序和语法、句法等要素，将其仅仅看作是若干个词汇的集合，文档中每个单词的出现都是独立的，不依赖于其它单词是否出现。也就是说，文档中任意一个位置出现的任何单词，都不受该文档语意影响而独立选择的。例如有如下两个文档：

1：Bob likes to play basketball, Jim likes too.
2：Bob also likes to play football games.

基于这两个文本文档，构造一个词典:
Dictionary={1:'Bob',2:'like',3:'to',4:'play',5:'basketball',6:'also',7:'football',8:'games',9:'Jim',10:'to'}

&#8195;&#8195;这个词典一共包含10个不同的单词，利用词典的索引号，上面两个文档每一个都可以用一个10维向量表示（用整数数字0-n表示某个单词在文档中出现的次数）：

1：[1, 2, 1, 1, 1, 0, 0, 0, 1, 1]
2：[1, 1, 1, 1 ,0, 1, 1, 1, 0, 0]

&#8195;&#8195;从上述的表示中，可以很清楚地看出来，在文档表示过程中并没有考虑关键词的顺序，而是仅仅将文档看成是一些关键词出现的概率的集合(这是Bag-of-Words模型的缺点之一)，每个关键词之间是相互独立的，这样每个文档可以表示成关键词出现频率的统计集合，类似于直方图的统计表示。

参考资料：http://blog.csdn.net/wsj998689aa/article/details/47089153

####25.intercept term
####26.interpolation
####27.iqr interquartile range
####28.issues with platt scaline
####29.
####30.
####31.
####32.
####33.
####34.
####35.
####36.
####36.KNN  Neighborhood  Size（KNN邻域尺寸）:
by-wgw
![36](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-36.jpg?raw=true)
&#8195;&#8195;KNN邻域尺寸，指的就是K的取值大小。中国有句古话，近朱者赤，近墨者黑，孔子曰：德不孤必有邻，都是说相近的多少总是有些相似的。K近邻法就是基于这样的思想，以一个点的最近邻的K个点的特征来预测这个点的特征，那么当K越大时，所包围进来的点就越多，那么准确性就会越差，也即是偏差越大，但是方差会越小。反过来当K越小时，所包围进来的点就越少，那么准确性就会越高，也即是偏差越小，但是方差会变大。
####37.
####38.
####39.
####40.
####41.
####42.
####43.
####44.
####45.
####46.
####47.
####48.
####49.
####50.
####51.
####52.
####53.




####54.
####55.

####56.Manhattan Distance（曼哈顿距离）
by-zcj
![56](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-56.jpg?raw=true)
![56_1](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/zcj%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/56_1.jpeg?raw=true)
&#8195;&#8195;图中红线代表曼哈顿距离，绿色代表欧氏距离，也就是直线距离，而蓝色和黄色代表等价的曼哈顿距离。曼哈顿距离——两点在南北方向上的距离加上在东西方向上的距离，即$d(i,j)=|x_i-x_j|+|y_i-y_j|$。对于一个具有正南正北、正东正西方向规则布局的城镇街道，从一点到达另一点的距离正是在南北方向上旅行的距离加上在东西方向上旅行的距离，因此，曼哈顿距离又称为出租车距离。
&#8195;&#8195;應用：向 OLS 公式中分別添加一个系数惩罚项， 一個為歐式距離的懲罰項，另一個為曼哈頓距離。
![56_2](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/zcj%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/56_2.jpeg?raw=trueg)
&#8195;&#8195;这就是系数平方和的惩罚项与绝对值和的惩罚项之间的区别。在左图中，随着 λ 变化以及最小点的移动，平方惩罚项产生的切点一般不会落在坐标轴上。Β1 与 β2 都不为 0。相比之下，在右图中，绝对值和惩罚项产生的切点落在了 β2 的轴上。在 β2 轴上，β1=0。一个稀疏的系数向量相当于算法告诉你可以忽略一些因变量。


####57.
####58.
####59.
####60.
####61.
####62.
####63.
####64.
####65.
####66.
####67.
####68.
####69.
####70.
####71.
####72.
####73.
####74.
####75.
####76.
####77.
####78.
####79.
####80.
####81.
####82.
####83.
####84.
####85.
####86.
####87.
####88.
####89.
####90.
####91.
####92.
####93.
####94.
####95.
####96.
####97.
####98.
####99.
####100.One Hot encoding（独热编码）:

&#8195;&#8195;问题由来：在很多機器學習任务中，特征并不总是连续值，而有可能是分类值。
&#8195;&#8195;例如，考虑以下的三个特征：
["male", "female"]
["from Europe", "from US", "from Asia"]
["male", "from US", v"uses Internet Explorer"] 表示为[0, 1, 3]
["female", "from Asia", "uses Chrome"]表示为[1, 2, 1]
&#8195;&#8195;但是，即使转化为数字表示后，上述数据也不能直接用在我们的分类器中。因为，分类器往往默认数据是连续的（可以计算距离），并且是有序的（而上面这个0并不是说比1要高级）。但是，按照我们上述的表示，数字并不是有序的，而是随机分配的。
&#8195;&#8195;为了解决上述问题，其中一种可能的解决方法是采用独热编码（One-Hot Encoding）。独热编码即 One-Hot 编码，又称一位有效编码，其方法是使用N位状态寄存器来对N个状态进行编码，每个状态都由他独立的寄存器位，并且在任意时候，其中只有一位有效。
&#8195;&#8195;例如：
自然状态码为：000,001,010,011,100,101；
独热编码为：000001,000010,000100,001000,010000,100000；
&#8195;&#8195;可以这样理解，对于每一个特征，如果它有m个可能值，那么经过独热编码后，就变成了m个二元特征（如成绩这个特征有好，中，差变成one-hot就是100, 010, 001）。并且，这些特征互斥，每次只有一个激活。因此，数据会变成稀疏的。
&#8195;&#8195; 这样做的好处主要有：
&#8195;&#8195; 1.解决了分类器不好处理属性数据的问题。2.在一定程度上也起到了扩充特征的作用。
举例：
基于Scikit-learn的例子：
```
from sklearn import preprocessing
enc = preprocessing.OneHotEncoder()
enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])
enc.transform([[0, 1, 3]]).toarray()
```
输出结果：
```
array([[ 1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.]])
```
注意: fit了4个数据3个特征，而transform了1个数据3个特征。第一个特征两种值(0: 10, 1: 01)，第二个特征三种值(0: 100, 1: 010, 2: 001)，第三个特征四种值(0: 1000, 1: 0100, 2: 0010, 3: 0001)。所以转换[0, 1, 3]为[ 1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.]。
參考資料：https://blog.csdn.net/pipisorry/article/details/61193868

####101.
####102.
####103.
####104.
####105.Out-Of-Bag  Error（袋外损失）:
by-wgw
![105](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-105.jpg?raw=true)
&#8195;&#8195;Out-Of-Bag Error也称为OOB损失或者袋外损失。在随机森林模型中袋外损失起到了测试集测试的功能。袋外损失是泛化损失的无偏估计。在用Bootstrap方法构建随机森林时，会将数据集随机抽样出三分之二用于构建一棵决策树。剩余的三分之一可以起到测试集的作用，但是这里与测试集有所区别，一棵树的测试集可能是另一棵树的训练集。Breiman发现，用袋外数据得出的损失，是泛化的误差的无偏估计，也就是说，袋外损失可以起到表征泛化误差的功能。
####106.
####107.
####108.Overfit vs Underfit（过拟合与欠拟合）:
by-zcj
![108](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-108.jpg?raw=true)
&#8195;&#8195;过拟合：简单理解就是训练样本的得到的输出和期望输出基本一致，但是测试样本输出和测试样本的期望输出相差却很大。为了得到一致假设而使假设变得过度复杂称为过拟合。想像某种学习算法产生了一个过拟合的分类器，这个分类器能够百分之百的正确分类样本数据（即再拿样本中的文档来给它，它绝对不会分错），但也就为了能够对样本完全正确的分类，使得它的构造如此精细复杂，规则如此严格，以至于任何与样本数据稍有不同的文档它全都认为不属于这个类别！
&#8195;&#8195;欠拟合：如果数据本身呈现二次型，故用一条二次曲线拟合会更好。但普通的PLS程序只提供线性方程供拟合之用。这就产生拟合不足即“欠拟合”现象，从而在预报时要造成偏差。如果我们用人工神经网络拟合，则因为三层人工神经网络拟合能力极强，有能力拟合任何函数。如果拟合彻底，就会连实验数据点分布不均匀，实验数据的误差等等“噪声”都按最小二乘判据拟合进数学模型。这当然也会造成预报的偏差。这就是“过拟合”的一个实例了。
参考资料：https://blog.csdn.net/cindysuna/article/details/50057579

####109.
####110.
####111.
####112.


####113.Bias（偏差）:
by-wgw
![113](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-113.jpg?raw=true)
偏差，是模型与真实值之间的期望误差。$$ Bias[\hat{f}(x)]=E[\hat{f}(x)-f(x)]$$其中$\hat{f}(x)$是模型的预测值。$f(x)$是真实值。预测值与真实值的期望误差，就是偏差。
####114.
####115.

####116.Partial Derivative（偏导数）：
-by zcj
![116](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-116.jpg?raw=true)

&#8195;&#8195;在数学中，一个多变量的函数的偏导数，就是保持其他变量恒定下关于其中一个变量的导数而（相对于全导数，在其中所有变量都允许变化）。偏导数在向量分析和微分几何中用途广泛。

1.引入
&#8195;&#8195;在一元函数中，导数就是函数的变化率。对于二元函数研究它的“变化率”，由于自变量多了一个，情况就要复杂的多。在 xOy 平面内，当动点由$P(x_0,y_0)$沿不同方向变化时，函数 f(x,y) 的变化快慢一般说来是不同的，因此就需要研究 f(x,y) 在$P(x_0,y_0)$点处沿不同方向的变化率。
在这里我们只学习函数 f(x,y) 沿着平行于 x 轴和平行于 y 轴两个特殊方位变动时， f(x,y) 的变化率。
偏导数的表示符号为:∂。
偏导数反映的是函数沿坐标轴正方向的变化率。
2.定义
&#8195;&#8195;x方向的偏导：设有二元函数 z=f(x,y) ，点$(x_0,y_0)$是其定义域D内一点。把y固定在$y_0$而让x在$x_0$ 有增量$\Delta_x$ ，相应地函数 z=f(x,y) 有增量（称为对 x 的偏增量）
$$\Delta_z=f(x_0+\Delta_x,y_0)-f(x_0,y_0)$$
&#8195;&#8195;如果$\Delta_z$与$\Delta_x$之比当 $\Delta_x\to0$ 时的极限存在，那么此极限值称为函数 z=f(x,y)在$(x_0,y_0)$处对 x 的偏导数，记作 $f^\prime_ x(x_0,y_0)$。函数 z=f(x,y)在$(x_0,y_0)$处对x的偏导数，实际上就是把y固定在$y_0$看成常数后，一元函数$z=f(x,y_0)$在$x_0$处的导数。
&#8195;&#8195;y方向的偏导：同样，把x固定在$x_0$，让y有增量$\Delta_y$，如果极限存在那么此极限称为函数 z=(x,y) 在 $(x_0,y_0)$处对y的偏导数。记作
$f^\prime_y(x0,y0)$。
3.求法
&#8195;&#8195;当函数 z=f(x,y) 在$(x_0,y_0)$的两个偏导数$ f^\prime_x(x_0,y_0)$与$ f^\prime_y(x_0,y_0)$都存在时，我们称 f(x,y) 在$(x_0,y_0)$处可导。如果函数 f(x,y) 在域D的每一点均可导，那么称函数 f(x,y) 在域D可导。此时，对应于域 D 的每一点 (x,y) ，必有一个对 x (对 y )的偏导数，因而在域 D 确定了一个新的二元函数，称为 f(x,y) 对 x (对 y )的偏导函数。简称偏导数。按偏导数的定义，将多元函数关于一个自变量求偏导数时，就将其余的自变量看成常数，此时他的求导方法与一元函数导数的求法是一样的。
4.几何意义
&#8195;&#8195;表示固定面上一点的切线斜率。偏导数$ f^\prime_x(x_0,y_0)$表示固定面上一点对 x 轴的切线斜率；偏导数$ f^\prime_y(x_0,y_0)$表示固定面上一点对 y 轴的切线斜率。
高阶偏导数：如果二元函数 z=f(x,y) 的偏导数$ f^\prime_x(x_0,y_0)$与$ f^\prime_y(x_0,y_0)$ 仍然可导，那么这两个偏导函数的偏导数称为 z=f(x,y) 的二阶偏导数。二元函数的二阶偏导数有四个：$f^{\prime\prime}_{xx},f^{\prime\prime}_{xy},f^{\prime\prime}_{yx},f^{\prime\prime}_{yy}$。
&#8195;&#8195;注意：$f^{\prime\prime}_{xy}$ 与 $f^{\prime\prime}_{yx}$的区别在于：前者是先对 x 求偏导，然后将所得的偏导函数再对 y 求偏导；后者是先对 y 求偏导再对 x 求偏导。当$f^{\prime\prime}_{xy}$ 与 $f^{\prime\prime}_{yx}$都连续时，求导的结果与先后次序无关。

补充：
如何理解导数的概念 ? https://www.zhihu.com/question/28684811
参考资料：
https://baike.baidu.com/item/%E5%81%8F%E5%AF%BC%E6%95%B0/5536984?fr=aladdin
####117.
####118.

####119.perceptron（感知机）：
by-zcj
![119](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-119.jpg?raw=true)
&#8195;&#8195;在机器学习中，感知机（Perceptron）是二分类的线性分类模型，属于监督学习算法。输入为实例的特征向量，输出为实例的类别（取+1和-1）。感知机对应于输入空间中将实例划分为两类的分离超平面。感知机旨在求出该超平面，为求得超平面导入了基于误分类的损失函数，利用梯度下降法对损失函数进行最优化。感知机的学习算法具有简单而易于实现的优点，分为原始形式和对偶形式。感知机预测是用学习得到的感知机模型对新的实例进行预测的，因此属于判别模型。感知机由Rosenblatt于1957年提出的，是神经网络和支持向量机的基础。
定义：假设输入空间(特征向量)为$X \subseteq R_n$，输出空间为Y={-1, +1}。输入$ x\in X$表示实例的特征向量，对应于输入空间的点；输出y∈Y表示示例的类别。由输入空间到输出空间的函数为$f(x)=sign(w·x+b)$称为感知机。其中，参数w为权值向量weight，b称为偏置bias。w·x表示w和x的点积。sign为符号函数，即
$$ \sum_{i=1}^{m}w_ix_i=w_1x_1+w_2x_2+···+w_n x_n$$
一种线性分类模型，属于判别模型。我们需要做的就是找到一个最佳的满足w⋅x+b=0的w和b值，即分离超平面（separating hyperplane）。
学习策略
核心：极小化损失函数。如果训练集是可分的，感知机的学习目的是求得一个能将训练集正实例点和负实例点完全分开的分离超平面。为了找到这样一个平面（或超平面），即确定感知机模型参数w和b，我们采用的是损失函数，同时将损失函数极小化。对于损失函数的选择，我们采用的是误分类点到超平面的距离（可以自己推算一下，这里采用的是几何间距，就是点到直线的距离）：
$$ \frac{1}{||w||}|w·x_0+b |$$
其中||w||是L2范数。
对于误分类点$(x_i,y_i)$来说：$$-y_i(w·x_i+b)>0$$
误分类点到超平面的距离为：$$ -\frac{1}{||w||}y_i(w·x_0+b)$$
那么，所有点到超平面的总距离为：$$ -\frac{1}{||w||}\sum_{x_i \in M}y_i(w·x_0+b)$$
不考虑$\frac{1}{ ||w|| }$，就得到感知机的损失函数：$$L(w,b)= -\sum_{x_i \in M}y_i(w·x_0+b)$$
其中M为误分类的集合。这个损失函数就是感知机学习的经验风险函数。可以看出，损失函数L(w,b)是非负的。如果没有误分类点，则损失函数的值为0，而且误分类点越少，误分类点距超平面的距离之和就越小，损失函数值就越小。同时，损失函数L(w,b)是连续可导函数。

学习算法：
感知机学习转变成求解损失函数L(w,b)的最优化问题。最优化的方法是随机梯度下降法（stochastic gradient descent），这里采用的就是该方法。说明一下，梯度下降其实是局部最优。感知机学习算法本身是误分类驱动的，因此我们采用随机梯度下降法。首先，任选一个超平面w0和b0，然后使用梯度下降法不断地极小化目标函数。极小化过程不是一次使M中所有误分类点的梯度下降，而是一次随机的选取一个误分类点使其梯度下降。
参考资料：https://blog.csdn.net/dream_angel_z/article/details/48915561

####120.
####121.
####122.
####123.
####124.
####125.
####126.
####127.
####128.
####129.
####130.
####131.
####132.
####133.
####134.
####135.
####136.
####137.
####138.
####139.
####140.
####141.
####142.Ridge Regression（岭回归）:
by-wgw
![142](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-142.jpg?raw=true)
&#8195;&#8195;岭回归，是对最小二乘法，也就是一般线性回归的优化，通过增加惩罚项，虽然损失了一点精确性，但是模型的稳定性更好，泛化能力更好了。
普通的线性回归会遇到一些问题，主要表现在：
假设样本的数量是n，特征的数量是p，
当n>>p时，最小二乘回归会有较小的方差，
当n～p时，容易产生过拟合，
当n<p时，最小二乘回归得不到有意义的结果。
岭回归就是在平方误差的基础上增加L2正则项：
$$ RSS+\lambda\sum_{j=1}^{p}\hat{B}_j^2$$
其中$\lambda$是调节参数，通过确定的值可以使得在方差和偏差之间达到平衡：随着$\lambda$的增大，模型方差减小而偏差增大。

####143.
####144.
####145.
####146.
####147.
####148.
####149.
####150.
####151.
####152.
####153.
####154.
####155.

####156.Softmax Normalization:
by-zcj
![](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-156.jpg?raw=true)
&#8195;&#8195;Softmax函数的本质就是将一个K 维的任意实数向量压缩（映射）成另一个K维的实数向量，其中向量中的每个元素取值都介于（0，1）之间。
$$\sigma:R^k \rightarrow (0,1)^K$$
$$\sigma(z)_j=\frac{e^{z_j}}{\sum_{k=1}^{K}e^{z_k}} for j=1,...,K.$$
![156_1](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/zcj%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/156_1.jpeg?raw=true)

&#8195;&#8195;举例：如果我们输入[1，2，3，4，1，2，3]，那么softmax是[0.024，0.064，0.175，0.475，0.024，0.064，0.175]。输出的大部分重量都是原始输入中的'4'。这是该功能通常用于：突出显示最大值并抑制显着低于最大值的值。但是请注意：softmax不是尺度不变的，所以如果输入[0.1,0.2,0.3,0.4,0.1,0.2,0.3]（其总和为1.6），softmax将是[0.125,0.138,0.1513,0.169,0.125， 0.138，0.153]。这表明，对于0到1之间的值，softmax事实上不重视最大值（注意，0.169不仅小于0.475，它也小于初始值0.4）。
&#8195;&#8195;Softmax标准化是一种减少数据中极端值或异常值的影响的方法，无需从数据集中移除它们。给出离群数据非常有用，我们希望将其包含在数据集中，同时仍将数据的重要性保留在均值的标准差内。
参考资料：
https://en.wikipedia.org/wiki/Softmax_function#Softmax_normalization
https://blog.csdn.net/u014422406/article/details/52805924
https://www.zhihu.com/question/23765351
####157.Brier Score（布莱尔分数）:
by-wgw
![157](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-157.jpg?raw=true)
&#8195;&#8195;布莱尔分数，是观测值的预测概率与实际结果直接的差的平方的平均值。
$$BS=\frac{1}{n}\sum_{t=1}^{n}(P_t-O_t)^2$$
其中$P_t$是预测概率，$O_t$是实际输出。
布莱尔分数在0到1之间。平方保证了结果为正值，布莱尔分数越低说明模型的预测效果越好。
####158.
####159.
####160.
####161.
####162.
####163.
####164.
####165.
####166.
####167.
####168.
####169.
####170.
####171.
####172.$AdjustedR^2$ ( 调整$R^2$ ) :
-by zcj
![172](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-112.jpg?raw=true)
&#8195;&#8195;当因变数Y 与自变数 X 之间的关系可以用一个回归方程式来解释时，X的解释能力有必要进一步地了解。该解释能力的程度大小，即回归分析的配合度(goodness of  fit)，是以决定系数(R2)来描述。如图一所示，总变异$(Y-\overline{Y})$是由两个成分组成：配合值离平均值之变异$(\hat{Y}-\overline{Y})$，以及观测值离配合值之变异$(Y-\hat{Y})$。前者$(\hat{Y}-\overline{Y})$是由所建立回归式之X 所引起，而后者$(Y-\hat{Y})$则为逢机机差所引起。由建立回归式之 X 所引起之平方和占总变异平方和的比例，称为决定系数即$R^2=\frac{\sum(\hat{Y}-\overline{Y})^2}{\sum(Y-\hat{Y})^2}=\frac{SSR}{SST}$ ；有些教科书则将$R^2$翻译为判定系数。$R^2$的所在范围在0与1之间，其结果的大小表示Y的变异中X所能解释的程度。当$R^2$值越接近1时，表示估计式中大部份Y之变异是由X影响而来，也代表利用X来解释Y的能力越强，因此所建立的回归模式为合适可接受。
![172_1](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/zcj%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/172_1.jpeg?raw=true)
&#8195;当自变数不只一项时，例如同时探讨氮、磷、钾多种肥料对作物产量的影响关系，或气温、日照、雨量等各种气象因素对产量的综合影响，一组自变数$X_1,X_2,X_3,…$与因变数Y之间的直线关系，可以利用复回归方程式(multiple regression function)来表示：$\hat{Y}=b_0+b_1X_1+b_2X_2+b_3X_3+····$。但并非所有生物现象都是呈直线关系，当肥料用量增加时，作物产量可能会以一缓慢的速度增加，以致于该曲线会逐渐平稳而接近水平；当过度施肥时，甚至对作物造成毒害而使曲线下降。因此有时候，非直线回归方程式的探讨，是有其必要性的。对同一套试验资料而言，到底应该适合于何种特定形式的回归方程式，常常也是我们探讨的重点。而此时，$R^2$是用来作为判断回归方程式是否有效的一个重要指标。也就是说，回归系数是看个别自变数与因变数间的净关系，而决定系数则是看全部自变数与因变数间的综合关系。
&#8195;&#8195;修正的公式是:
$$R^{2}_{adj}=\frac{(n-1)(1-R^2)}{n-p-1}$$
&#8195;&#8195;其中n是样本数量，p是模型中变量的个数。
&#8195;&#8195;我们知道在其他变量不变的情况下，引入新的变量，总能提高模型的$R^2$。$R^2_{adj}$就是相当于给变量的个数加惩罚项。换句话说，如果两个模型，样本数一样，$R^2$一样，那么从$R^2_{adj}$的角度看，使用变量个数少的那个模型更优。

参考资料：
http://ilc.hk.edu.tw/c/document_library/get_file?p_l_id=260741&folderId=261080&name=DLFE-3350.pdf
http://sofasofa.io/forum_main_post.php?postid=1000702

####173.

####174.Supervised vs Unsupervised（有监督和无监督）:
![174](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-174.jpg?raw=true)
&#8195;&#8195;机器学习的类型有两种：有监督学习和无监督学习。
&#8195;&#8195;有监督学习：在有监督学习中，我们有一些训练数据。使用这些训练数据，我们创建了一个预测模型，可用于未来预测结果。最终目标是建立一个称为假设函数的预测函数f(x)。 “学习”使用数学算法对该函数进行优化，以便给定输入数据x关于某个域，它将能够预测一些有趣的值f(x)。例如，房子的市场价格。属于此类别的一些算法是回归、分类、KNN、决策树、朴素湾、支持向量机等。
&#8195;&#8195;示例：假设你提供了装满不同水果的篮子，你的任务是对它们进行分组。假设这些水果是苹果、香蕉、西瓜、橙子。既然你熟悉水果，你可以很容易地将它们分成苹果、香蕉、西瓜、橙子等各种类别。这项活动被称为训练数据。现在，如果您获得不同类似的水果，您将在未来使用此学习体验。这种类型的算法被称为分类。
&#8195;&#8195;无监督学习：与有监督学习不同，在无监督情况下，我们拥有带有输入x的训练数据集，但没有任何目标值或我们可以预测的内容。此信息缺失。这种学习的目标是在数据集内找出一组类似的例子。这种算法被称为聚类。依上例，现在让我们说，如果你再次提供了装满水果的桶，但这次你不知道那些水果。你以前从未见过他们。然后你将使用身体特征将他们放在不同的组中。比方说颜色，大小等，这称为聚类。因此，无监督学习任务涉及识别数据内的关系。这里没有提供培训示例。比如用K-means聚类算法。
参考资料：https://www.linkedin.com/pulse/machine-learning-supervised-vs-unsupervised-amit-sinha
####175.
####176.
####177.
####178.
####179.
####180.
####181.
####182.
####183.
####184.
####185.

####186.Tht Effect Of Feature Scaling On Gradient Descent:
by-wgw
![186](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-186.jpg?raw=true)
&#8195;&#8195;特征缩放在梯度下降中的效果，在梯度下降算法中，存在这样一个问题，当不同的特征的范围差距过大时，模型的预测会变得困难很多，代价函数的轮廓图会非常的偏斜，下降最快的方向与全局下降最快的方向往往不一致，直观图如下所示：
![186_1](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/wgw%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/186_1.jpeg?raw=true)

经过特征缩放之后，代价函数的轮廓图就会变的相对标准，下降最快的方向与全局下降最快的方向几乎一致：
![186_2](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/wgw%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/186_2.jpeg?raw=true)
特征缩放的一个最简答的方法就是将特征映射到0-1之间：
$$x^{,}=\frac{x-x_{min}}{x_{max}-x_{min}}$$
将每一个特征都代入就可以实现特征的一致化，也就是特征的缩放。

####187.

####188.The Random In Random Forest（随机森林中的随机）:
by-wgw
![188](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-188.jpg?raw=true)
随机森林中的随机,体现在：
1，每一棵决策树的训练集都是在总训练集中有放回的随机抽样出的的随机样本，称为袋内数据，每一棵树的袋外数据作为这棵树的测试集。
2，每一棵树都拿到所有的特征，但是每一个节点只有这些特征的一部分子集是可用的。

####189.
####190.
####191.
####192.
####193.
####194.
####195.
####196.
####197.
####198.
####199.
####200.
####201.
####202.
####203.
####204.
####205.
####206.
####207.
####208.
####209.
####210.
####211.
####212.
####213.
####214.
####215.
####216.
####217.

####218.Why Is It Called A Cost Function（成本函数）:
by-wgw
![218](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-218.jpg?raw=true)
&#8195;&#8195;成本函数，是用一个模型代替真实世界的“成本”。这里借用了经济学里面的成本概念。成本函数可以认为是模型值与真实值之间的差异的度量。成本函数是我们要尽量最小化的。之所以叫做成本函数，是因为用模型代替世界产生的差异的确就像是我们要承担的成本一样。

####219.
####220.
####221.
####222.

####223.Agglometrative Clustering:
![223](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-223.jpg?raw=true)
&#8195;&#8195;Agglometrative Clustering,简写为AGNES，是一种自下而上的聚类算法，是层次聚类的两种方法的其中一种，另一种采用的是自上而下的分拆方法。AGNES的方法是将数据集中的点按照距离分类，这里的距离有很多种，最简单的可以是欧氏距离。AGNES聚类的步骤如下：
第一步，将所有的数据点单独为类，也就是一万个数据点那就有一万个类，
第二步，不断的合并类，这里合并的依据是类与类之间的距离。
第三步，直到某一个指标达到，则停止。如果不停止，那么所有的数据点都会聚为同一个类。
这里比较值得注意的点是，第二步里，类与类之间的距离的确定。
一般有三种方法确定三种不同定义的距离：
对于类$C_i$与$C_j$,$x_i$属于$C_i$,$x_j$属于$C_j$

1最小距离：类与类之间的距离最小的数据点的距离作为类之间的距离。
数学表示为：$$ d_{min}=(C_i,C_j)=min   distance(x_i,x_j) $$

2最大距离：类与类之间的距离最大的数据点的距离作为类之间的距离。数学表示为：
$$d_{max}=(C_i,C_j)=max    distance(x_i,x_j)$$
3平均距离：类与类的所有数据点的平均距离作为类之间的距离。数学表示为：$$d_{avg}=(C_i,C_j)=\frac{1}{|C_i||C_j|}\sum_{x\in C_i} \sum_{x\in C_j}distance(x_i,x_j)$$
一般情况下，最小距离和最大距离法都由于比较“偏激”而被较少使用，使用最多的是平均距离。
用图形形象的表示层次聚类法如下：
![223_1](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/wgw%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/223_1.png?raw=true)

在最下层，每一个数据单独为类，分类距离越大，则多个类会被划分到同一个类中去。

####224.
####225.
####226.
####227.
####228.
####229.
####230.
####231.
####232.
####233.
####234.
####235.
####235.AIC
--by zcj
![](https://github.com/GaoWeio/300-Concepts-of-Machine-Learning/blob/master/4_AIC.png?raw=true)

 &nbsp; 很多参数估计问题均采用似然函数作为目标函数，当训练数据足够多时，可以不断提高模型精度，但是以提高模型复杂度为代价的，同时带来一个机器学习中非常普遍的问题——过拟合。所以，模型选择问题在模型复杂度与模型对数据集描述能力（即似然函数）之间寻求最佳平衡。

$$123$$

  &#8195;&#8195;人们提出许多信息准则，通过加入模型复杂度的惩罚项来避免过拟合问题，此处我们介绍一下常用的两个模型选择方法——赤池信息准则（Akaike Information Criterion，AIC）和贝叶斯信息准则（Bayesian Information Criterion，BIC）。
&#8195;&#8195;AIC是衡量统计模型拟合优良性的一种标准，由日本统计学家赤池弘次在1974年提出，它建立在熵的概念上，提供了权衡估计模型复杂度和拟合数据优良性的标准。
&#8198;&#8198;通常情况下，AIC定义为：
$$AIC=2k-2ln(L)$$
&#8195;&#8195;其中k是模型参数个数，L是似然函数。从一组可供选择的模型中选择最佳模型时，通常选择AIC最小的模型。
&#8195;&#8195;当两个模型之间存在较大差异时，差异主要体现在似然函数项，当似然函数差异不显著时，上式第一项，即模型复杂度则起作用，从而参数个数少的模型是较好的选择。
&#8195;&#8195;一般而言，当模型复杂度提高（k增大）时，似然函数L也会增大(即RSS變小)，从而使AIC变小，但是k过大时，似然函数增速减缓，导致AIC增大，模型过于复杂容易造成过拟合现象。目标是选取AIC最小的模型，AIC不仅要提高模型拟合度（极大似然），而且引入了惩罚项，使模型参数尽可能少，有助于降低过拟合的可能性。
参考资料：http://blog.csdn.net/lynnucas/article/details/47947943

####236.
####237.
####238.
####239.
####240.
####241.
####242.
####243.
####244.

####245.Decision Boundary（决策边界）：
-by zcj
![245](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-245.jpg?raw=true)

&#8195;&#8195;决策边界就是能够把样本正确分类的一条边界，主要有线性决策边界(linear decision boundaries)和非线性决策边界(non-linear decision boundaries)。注意：决策边界是假设函数的属性，由参数决定，而不是由数据集的特征决定。下面主要举一些例子，形象化的来说明线性决策边界和非线性决策边界。先看一个线性决策边界的例子：（注：图片来源：ng的machine learning课）

![245_1](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/zcj%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/245_1.jpeg?raw=true)

&#8195&#8195再来看一个非线性决策边界的例子：

![245_2](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/zcj%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/245_2.jpeg?raw=true)
&#8195;&#8195;假设我们的数据呈现出如上图的分布情况，那么我们的模型是什么样才能适合这些数据呢？我们需要的是一个二次方特征。由不同机器学习算法可以得到不同的决策边界，不同的数据或不同的算法都会影响边界的准确性。以下图为例，紫色虚线是贝叶斯决策边界线，黑色实线则是KNN的分类边界，由图可以发现对于该组数据贝叶斯决策边界线是较理想的分类边界。
![245_3](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/zcj%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/245_3.jpeg?raw=true)
参考资料：http://blog.csdn.net/u012328159/article/details/51068427

####246.
####247.
####248.
####249.
####250.
####251.
####252.
####253.
####254.
####255.
####256.
####257.
####258.
####259.
####260.
####261.
####262.
####263.
####264.
####265.
####266.
####267.
####268，Anscombe\'s Quartet（安斯库姆四重奏）：
by-wgw
![268](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-268.jpg?raw=true)
&#8195;&#8195;安斯库姆四重奏是四组具有完全相同的统计特征的数据，这些统计特征包括均值，方差，回归曲线方程等，然而，当把这四组数据以散点图的形式展示出来时会发现这四组数据是极不相同的。安斯库姆四重奏表明，仅仅用简单的统计特征量表述数据是会遗漏某些反应了数据本质的其他重要特征的，在应用数据前对数据进行可视化是非常重要的。当然，安斯库姆四重奏是为了说明统计描述的局限性以及数据可视化的重要性，而精心设计出的四组数据，在现实中这样的情况并不多见。
安斯库姆四重奏散点图：
![268_1](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/wgw%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/268_1.jpeg?raw=true)
安斯库姆四重奏统计特征：
![268_2](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/wgw%E5%BC%95%E7%94%A8%E5%9B%BE%E7%89%87/268_2.jpeg?raw=true)

####269.
####270.
####271.

####272.F1 Score（F1分数）：
-by zcj
![272](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-272.jpg?raw=true)

&#8195;&#8195;F1分数（F1 Score），是统计学中用来衡量二分类模型精确度的一种指标。它同时兼顾了分类模型的准确率和召回率。F1分数可以看作是模型准确率和召回率的一种加权平均，它的最大值是1，最小值是0。
&#8195&#8195数学定义:F1分数（F1Score），又称平衡F分数（Balanced F Score），它被定义为精确率和召回率的调和平均数。
$$F_1=2\frac{precision·recall}{precision+recall}$$
更一般的，我们定义$F_\beta$分数为
$$F_\beta=(1+\beta^2)\frac{precision·recall}{\beta^2·precision+recall}$$

&#8195;&#8195; 物理意义：人们通常使用准确率和召回率这两个指标，来评价二分类模型的分析效果。但是当这两个指标发生冲突时，我们很难在模型之间进行比较。比如，我们有如下两个模型A、B，A模型的召回率高于B模型，但是B模型的准确率高于A模型，A和B这两个模型的综合性能，哪一个更优呢？
|   模型    |    准确率  |  召回率 |
|  :--:    |    :--:   |   :--: |
| A        |      80%  |   90%  |
| B        |      90%  |   80%  |





&#8195;&#8195;为了解决这个问题，人们提出了$F_B$分数。FB的物理意义就是将准确率和召回率这两个分值合并为一个分值，在合并的过程中，召回率的权重是准确率的B倍。F1分数认为召回率和准确率同等重要，F2分数认为召回率的重要程度是准确率的2倍，而F0.5分数认为召回率的重要程度是准确率的一半。
&#8195;&#8195;应用领域：F分数被广泛应用在信息检索领域，用来衡量检索分类和文档分类的性能。早期人们只关注F1分数，但是随着谷歌、百度等大型搜索引擎的兴起，召回率和准确率对性能影响的权重开始变得不同，人们开始更关注其中的一种，所以$F_B$分数得到越来越广泛的应用。F分数也被广泛应用在自然语言处理领域，比如命名实体识别、分词等，用来衡量算法或系统的性能。

补充：
1. G分数是另一种统一准确率和召回率的系统性能评估标准。F分数是准确率和召回率的调和平均数，G分数被定义为准确率和召回率的几何平均数。

2. 精确率、召回率、F1 值、ROC、AUC 各自的优缺点是什么?
https://www.zhihu.com/question/30643044

参考资料：https://baike.baidu.com/item/F1%E5%88%86%E6%95%B0/13864979?fr=aladdin

####273.
####274.
####275.
####276.
####277.
####278.
####279.

####280.Fowlkes-Mallows-Score（福克斯-马洛斯得分）：
by-wgw
![280](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-280.jpg?raw=true)
&#8195;&#8195;福克斯-马洛斯得分也称$FMI$得分，是对聚类模型的性能度量的一种方法。对于给定数据集，参考模型和测试模型分别给出的了簇划分。对于任意一对数据的划分，分为四种情况：
1，参考模型划入同一簇，测试模型也划入同一簇。以TP（True Positive）表示。
2，参考模型划入同一簇，测试模型未划入同一簇。以FN（False Negative）表示。
3，参考模型未划入同一簇，测试模型划入同一簇。以FP（False Positive）表示。
4，参考模型未划入同一簇，测试模型也未划入同一簇。以TN（True Negative）表示。
则FMI得分是：$$FMI=\frac{TP}{\sqrt{(TP+FN)(TP+FP)}}$$
FMI得分越高，说明测试模型与参考模型越接近。

####281.
####282.
####283.
####284.
####285.Gini-Index(基尼指数)：
by-wgw
![285](https://github.com/6studentsfromsspku/sspku-300-concepts/blob/master/%E5%8D%A1%E7%89%87%E9%9B%86%E5%90%88/%E5%8D%A1%E7%89%87-285.jpg?raw=true)
&#8195;&#8195;在决策树模型中，需要确定哪些分类的属性作为分枝节点。我们选择那些使得数据集每一次分类后，子集的纯度最高的属性。基尼指数就是指示数据集纯度的指标。对于第m个节点，根据属性I数据集D可以分为k类，那么基尼指数为：
$$Gini(D_i)=\sum_{k=1}^kp_{mk}(1-p_{mk})$$
&#8195;&#8195;当$p_{mk}$越接近$0$或$1$时，$Gini(D_i)$越小，也表示数据集的纯度越高，相应的属性$I$就越适合模型，当$p_{mk}$越接近$\frac{1}{2}$时，$Gini(D_i)$越大，也表示数据集$I$的纯度越低相应的属性$I$也就越不适合模型。
####286.
####287.
####288.
####289.
####290.AUC
-by zcj


![Alt text](./26_AUC Area Under The Curve.jpeg)


&#8195&#8195ROC（Receiver Operating Characteristic）曲线和AUC常被用来评价一个二值分类器（binary classifier）的优劣，对两者的简单介绍见这里。这里简单介绍ROC和AUC的特点，以及如何作出ROC曲线图并计算AUC。

ROC曲线
&#8195&#8195对于分类器（这里讨论二值分类），或者说分类算法，评价指标主要有precision、recall、F1-score、以及ROC和AUC。下图是一个ROC曲线的示例。
![Alt text](./ROC曲线示例.jpeg)
<center>ROC曲线示例</center>

&#8195&#8195正如在这个ROC曲线的示例图中看到的那样，ROC曲线的横坐标为False positive rate（FP rate），纵坐标为True positive rate（TP rate）。下图中详细说明了FP和TP是如何定义的。
![Alt text](./FP_TP定义.jpeg)


&#8195&#8195接下来考虑ROC曲线图中的四个点和一条线。第一个点(0,1)，即FPR=0, TPR=1，这意味着FN（False negative）=0，并且FP（False positive）=0。这是一个完美的分类器，它将所有的样本都正确分类。第二个点，(1,0)，即FP rate=1，TP rate=0，类似地分析可以发现这是一个最糟糕的分类器，因为它成功避开了所有的正确答案。第三个点，(0,0)，即FPR=TPR=0，即FP（False positive）=TP（True positive）=0，可以发现该分类器预测所有的样本都为负样本（Negative）。类似的，第四个点（1,1），分类器实际上预测所有的样本都为正样本。经过以上的分析，我们可以断言，ROC曲线越接近左上角，该分类器的性能越好。
&#8195&#8195下面考虑ROC曲线图中的虚线y=x上的点。这条对角线上的点其实表示的是一个采用随机猜测策略的分类器的结果，例如(0.5,0.5)，表示该分类器随机对于一半的样本猜测其为正样本，另外一半的样本为负样本。

&#8195&#8195如何画ROC曲线：对于一个特定的分类器和测试数据集，显然只能得到一个分类结果，即一组FPR和TPR结果，而要得到一个曲线，我们实际上需要一系列FPR和TPR的值，这又是如何得到的呢？我们先来看一下Wikipedia上对ROC曲线的定义：In signal detection theory, a receiver operating characteristic (ROC), or simply ROC curve, is a graphical plot which illustrates the performance of a binary classifier system as its discrimination threshold is varied.
&#8195&#8195问题在于“as its discrimination threashold is varied”。如何理解这里的“discrimination threashold”呢？我们忽略了分类器的一个重要功能“概率输出”，即表示分类器认为某个样本具有多大的概率属于正样本（或负样本）。通过更深入地了解各个分类器的内部机理，我们总能想办法得到一种概率输出。通常来说，是将一个实数范围通过某个变换映射到(0,1)区间。
    假如我们已经得到了所有样本的概率输出（属于正样本的概率），现在的问题是如何改变“discrimination threashold”呢？我们根据每个测试样本属于正样本的概率值从大到小排序。下图是一个示例，图中共有20个测试样本，“Class”一栏表示每个测试样本真正的标签（p表示正样本，n表示负样本），“Score”表示每个测试样本属于正样本的概率。

![Alt text](./AUC_概率从大到小排序.jpeg)

&#8195&#8195接下来，我们从高到低，依次将“Score”值作为阈值threshold，当测试样本属于正样本的概率大于或等于这个threshold时，我们认为它为正样本，否则为负样本。举例来说，对于图中的第4个样本，其“Score”值为0.6，那么样本1，2，3，4都被认为是正样本，因为它们的“Score”值都大于等于0.6，而其他样本则都认为是负样本。每次选取一个不同的threshold，我们就可以得到一组FP rate和TP rate，即ROC曲线上的一点。这样一来，我们一共得到了20组FP rate和TP rate的值，将它们画在ROC曲线的结果如下图：

![Alt text](./AUC_False positive rate0.jpeg)

&#8195&#8195当我们将threshold设置为1和0时，分别可以得到ROC曲线上的(0,0)和(1,1)两个点。将这些(FPR,TPR)对连接起来，就得到了ROC曲线。当threshold取值越多，ROC曲线越平滑。
&#8195&#8195AUC（Area Under Curve）被定义为ROC曲线下的面积，显然这个面积的数值不会大于1。又由于ROC曲线一般都处于y=x这条直线的上方，所以AUC的取值范围在0.5和1之间。使用AUC值作为评价标准是因为很多时候ROC曲线并不能清晰的说明哪个分类器的效果更好，而作为一个数值，对应AUC更大的分类器效果更好。
&#8195&#8195在了解了ROC曲线的构造过程后，编写代码实现并不是一件困难的事情。相比自己编写代码，有时候阅读其他人的代码收获更多，当然过程也更痛苦些。在此推荐scikit-learn中关于计算AUC的代码。
&#8195&#8195那么AUC值的含义是什么呢？根据(Fawcett, 2006)，AUC的值的含义是：
The AUC value is equivalent to the probability that a randomly chosen positive example is ranked higher than a randomly chosen negative example.
首先AUC值是一个概率值，当你随机挑选一个正样本以及一个负样本，当前的分类算法根据计算得到的Score值将这个正样本排在负样本前面的概率就是AUC值。当然，AUC值越大，当前的分类算法越有可能将正样本排在负样本前面，即能够更好的分类。

&#8195&#8195既然已经有这么多评价标准，为什么还要使用ROC和AUC呢？因为ROC曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。在实际的数据集中经常会出现类不平衡（class imbalance）现象，即负样本比正样本多很多（或者相反），而且测试数据中的正负样本的分布也可能随着时间变化。下图是ROC曲线和Precision-Recall曲线的对比：

![Alt text](./AUC_ROC曲线和P-R曲线的对比.jpeg)

&#8195&#8195在上图中，(a)和(c)为ROC曲线，(b)和(d)为Precision-Recall曲线。(a)和(b)展示的是分类其在原始测试集（正负样本分布平衡）的结果，(c)和(d)是将测试集中负样本的数量增加到原来的10倍后，分类器的结果。可以明显的看出，ROC曲线基本保持原貌，而Precision-Recall曲线则变化较大。

参考资料：http://alexkong.net/2013/06/introduction-to-auc-and-roc/

####291.
####292.
####293.
####294.
####295.
####296.
####297.
####298.
####299.
####300.



以下为曾经所做，还未并入。













####?.The Argument For Parametric Models（）
当数据生成函数大致匹配参数概率分布时，我们可以将我们的计算仅限于其参数。这使得仅使用少量信息就知道很多。 许多概率分布的灵活性意味着如果选择分布族(举例：正态跟学生T)通常不是问题 然而，范围匹配是重要的。例如，如果我们想要一个概率，我们就不应该选择输出数大于1的概率分布。



####?.特征选择中的卡方应用
&#8195;在数据预处理的过程中，重要的一步是特征选择，也就是选择出与模型和算法最相互适应的特征。特征选择的方法有好几种，卡方检验是其中一种，卡方检验的做法是对特征与目标进行关联给出卡方值，卡方值从小到大排序，越靠前的特征效果越好。卡方检验由英国统计学家Pearson在1900年提出。
通用的卡方检验步骤：
1）假设命题$H_0$和$H_1$，$H_0$命题表示不符合假设的分布，$H_1$命题表示符合假设的分布。
2）计算卡方值，根据以下公式：$$\chi^2=\sum_1^n\frac{(O_i-E_i)}{E_i}$$对于两列数据，$O_i$表示实际值，$E_i$表示理论值，所谓理论值即是按照特定分布应取的期望值。$n-1$为自由度。
3）查卡方检验表：
![Alt text](./屏幕快照 2018-02-19 13.52.02.png)
4）计算出的$\chi^2$值如果小于查出的值，则拒绝$H_0$接受$H_1$，否则拒绝$H_1$接受$H_0$

####?，类别特征：
&#8195;&#8195;特征有两种，一种是有序的特征，比如学习科目成绩的高低、年龄的大小……等。另一种是无序的，比如性别，水果，城市的种类。对于有序的特征，本身的数值即可作为机器处理的对象。对于无序的特征，若要转化为机器可以直接处理的对象，需要数字化，但是这里的数字化不是简单的为无序特征赋值。
无序特征的数字化：
&#8195;&#8195;对于无序特征，例如对于性别男，性别女，分别赋值1，2。那么，1小于2是否意味着男小于女？显然不是的。但是机器会认为1是小于2的，这会干扰到机器学习的效果。所以对于无序特征的数字化，应避免数字化后，使得本来无序的特征有了顺序。
&#8195;&#8195;一个可行的方法是对无序特征赋于向量值。例如对性别为男赋值$[0,1]$.对性别为女赋值$[1,0]$。若有更多的类别，可类似的赋值$[0,0,0,1]$，$[0,0,1,0]$等。


####?.Variance for feature selection :
&#8195;&#8195;特征选择中的方差，方差是特征含有的信息的一个度量，方差越大，说明特征中含有的信息越多，方差越小，说明特征中含有的信息越少，方差越小，则特征越不能很好的训练一个模型，反过来方差越大，越能很好的训练出一个模型。所以在特征选择中，我们应尽量选择那些方差较大的特征。
